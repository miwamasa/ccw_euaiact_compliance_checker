metadata:
  title: EU AI Act Compliance Checker (EC Service Desk)
  version: '1.0'
  source: https://ai-act-service-desk.ec.europa.eu/en/eu-ai-act-compliance-checker
  updated: '2026-01-30'
  purpose: Determine obligations under the EU AI Act for AI systems and models
questionnaire:
  Q1:
    id: Q1
    question: Do you want to check an AI model or an AI system?
    info: 'The AI Act refers to AI models as ''essential components of AI systems, [yet] they do not constitute AI systems
      on their own. AI models require the addition of further components, such as for example a user interface, to become
      AI systems. AI models are typically integrated into and form part of AI systems'' (Recital 97). ''Large generative AI
      models are a typical example for a general-purpose AI model, given that they allow for flexible generation of content,
      such as in the form of text, audio, images or video, that can readily accommodate a wide range of distinctive tasks''
      (Recital 99).

      The AI Act defines AI systems as ''machine-based system that is designed to operate with varying levels of autonomy
      and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the
      input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence
      physical or virtual environments'' (Article 3 (1), Recital 12).

      If you wish to classify an AI system into which you have integrated your own AI model, use the Compliance Checker twice—once
      for the AI system and once for the AI model. The obligations under the AI Act for the general purpose AI model apply
      in addition to those for the AI system (Recital 97). Conversely, if you wish to classify your AI model, be aware that
      it may be incorporated into an AI system if you use it yourself or offer it as a service, in which case you may use
      the Compliance Checker twice, too.'
    type: single_choice
    sources: ''
    options:
    - value: '0'
      label: AI model
      help: The AI Act refers to AI models as 'essential components of AI systems, [yet] they do not constitute AI systems
        on their own. AI models require the addition of further components, such as for example a user interface, to become
        AI systems. AI models are typically integrated into and form part of AI systems' (Recital 97). 'Large generative AI
        models are a typical example for a general-purpose AI model, given that they allow for flexible generation of content,
        such as in the form of text, audio, images or video, that can readily accommodate a wide range of distinctive tasks'
        (Recital 99).
    - value: '1'
      label: AI system
      help: Your situation involves a machine-based system that is designed to operate with varying level of autonomy, and
        that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input
        it receives how to generate outputs such as predictions, content, recommendations, or decisions that can influence
        physical or virtual environments.
  QAIS 1.1:
    id: QAIS 1.1
    question: Does your AI system have these characteristics?
    info: ''
    type: multiple_choice
    sources: Article 3; Recital 12; [Guidelines on AI system definition](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-definition_en-1.pdf)
    options:
    - value: '0'
      label: The AI system is machine-based
      help: "Following the European Commission's guidelines on the definition of an AI system, the term ‘machine-based’ refers\
        \ to the fact that AI systems are developed with and run on machines. The term ‘machine’ can be understood to include\
        \ both the hardware and software components that enable the AI system to function. The hardware components refer to\
        \ the physical elements of the machine, such as processing units, memory, storage devices, networking units, and input/output\
        \ interfaces, which provide the infrastructure  for computation. The software components encompass computer code,\
        \ instructions,  programs, operating systems, and applications that handle how the hardware processes data and performs\
        \ tasks. \nAll AI systems are machine-based, since they require machines to enable their functioning, such as model\
        \ training, data processing, predictive modelling and large-scale automated decision making. The entire lifecycle\
        \ of advanced AI systems relies on  machines that can include many hardware or software components. The element of\
        \ ‘machine-based’ in the definition of AI system underlines the fact that AI systems must be computationally driven\
        \ and based on machine operations.   \nThe term ‘machine-based’ covers a wide variety of computational systems. For\
        \ example, the currently most advanced emerging quantum computing systems, which represent a significant departure\
        \ from traditional computing systems, constitute machine-based systems, despite their unique operational principes\
        \ and use of quantum-mechanical phenomena, as do biological or organic systems so long as they provide computational\
        \ capacity. For detailed information, please refer to the [guidelines on the definition of an AI system, Section 1,\
        \ paragraphs 11-13.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-definition_en-1.pdf#nameddest=para_11)"
    - value: '1'
      label: The AI system is designed to operate with varying levels of autonomy
      help: 'Following the European Commission''s guidelines on the definition of an AI system, central to the concept of
        autonomy is ‘human involvement’ and ‘human intervention’  and thus human-machine interaction. At one extreme of possible
        human-machine interaction are systems which are designed to perform all tasks though manually operated functions.
        At the other extreme are systems that are capable to operate without any human involvement or intervention, i.e. fully
        autonomously.

        The reference to ‘some degree of independence of action’ in recital 12 AI Act excludes systems that are designed to
        operate solely with full manual human involvement and intervention. Human involvement and human intervention can be
        either direct, e.g. through manual controls, or indirect, e.g. though automated systems-based controls which allow
        humans to delegate or supervise system operations. The reference in the definition of an AI system in Article 3(1)
        AI Act to ‘machine-based system that is designed to operate with the varying levels of autonomy’ underlines the ability
        of the system to interact with its external environment, rather than a choice of a specific technique, such as machine
        learning, or model architecture for the development of the system.

        For detailed information, please refer to the [guidelines on the definition of an AI system, Section 2, paragraphs
        14-21.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-definition_en-1.pdf#nameddest=para_14)

        '
    - value: '2'
      label: The AI system may exhibit adaptiveness after deployment
      help: "Following the European Commission's guidelines on the definition of an AI system, the concepts of autonomy and\
        \ adaptiveness are two distinct but closely related concepts. They are often discussed together but they represent\
        \ different dimensions of an AI system’s functionality. Recital 12 AI Act clarifies that ‘adaptiveness’ refers to\
        \ self-learning capabilities, allowing the behaviour of the system to change while in use. The new behaviour of the\
        \ adapted system may produce different results from the previous system for the same inputs.  \nThe use of the term\
        \ ‘may’ in relation to this element of the definition indicates that a system may, but does not necessarily have to,\
        \ possess adaptiveness or self-learning capabilities after deployment to constitute an AI system. Accordingly, a system’s\
        \ ability to automatically learn, discover new patterns, or identify relationships in the data beyond what it was\
        \ initially trained on is a facultative and thus not a decisive condition for determining whether the system qualifies\
        \ as an AI system. For detailed information, please refer to the [guidelines on the definition of an AI system, Section\
        \ 3, paragraphs 22-23.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-definition_en-1.pdf#nameddest=para_22)"
    - value: '3'
      label: The AI system infers from the input it receives, how to generate outputs for explicit or implicit objectives
        (e.g. explicit objectives refer to clearly stated goals that are directly coded/implicit objectives refer to goals
        that are not explicitly stated but may be deduced from the behavior or underlying assumptions of the system)
      help: "The capacity of an AI system to infer transcends basic data processing by enabling learning, reasoning or modelling.\
        \ Moreover, 'this capability to infer refers to the process of obtaining the outputs, such as predictions, content,\
        \ recommendations, or decisions, (…) and to a capability of AI systems to derive models or algorithms, or both, from\
        \ inputs or data. The techniques that enable inference while building an AI system include machine learning approaches\
        \ that learn from data how to achieve certain objectives, and logic- and knowledge-based approaches that infer from\
        \ encoded knowledge or symbolic representation of the task to be solved.' (Recital 12). \nFollowing the European Commission's\
        \ guidelines on the definition of an AI system, the ‘process of obtaining the outputs, such as predictions, content,\
        \ recommendations, or decisions, which can influence physical and virtual environments’, refers to the ability of\
        \ the AI system, predominantly in the ‘use phase’, to generate outputs based on inputs. A ‘capability of AI systems\
        \ to derive models or algorithms, or both, from inputs or data’ refers primarily, but is not limited to, the ‘building\
        \ phase’ of the system and underlines  the relevance of the techniques used for building a system. \nThe terms ‘infer\
        \ how to’, used in Article 3(1) and clarified in Recital 12 AI Act, is broader than, and not limited only to, a narrow\
        \ understanding of the concept of inference as an ability of a system to derive outputs from given inputs, and thus\
        \ infer the result. \nAccordingly, the formulation used in Article 3(1) AI Act, i.e. ‘infers, how to generate outputs’,\
        \ should be understood as referring to the building phase, whereby a system derives outputs through AI techniques\
        \ enabling inferencing. \nThe concept of ‘inference’ should be understood in a broader sense as encompassing the ‘building\
        \ phase’ of the AI system. Recital 12 AI Act then provides further guidance on techniques that enable this ability\
        \ of an AI system to infer how to generate outputs. Accordingly, the techniques that may be used to enable inference\
        \ include ‘machine learning approaches that learn from data how to achieve certain objectives and logic- and knowledge-based\
        \ approaches that infer from encoded knowledge or symbolic representation of the task to be solved.’\nThe first category\
        \ of AI techniques mentioned in Recital 12 AI Act is ‘machine learning approaches that learn from data how to achieve\
        \ certain objectives’. That category includes a large variety of approaches enabling a system to ‘learn’, such as\
        \ supervised learning, unsupervised learning, self-supervised learning and reinforcement learning. \nIn addition to\
        \ various machine learning approaches discussed above, the second category of techniques mentioned in Recital 12 AI\
        \ Act are ‘logic- and knowledge-based approaches that infer from encoded knowledge or symbolic representation of the\
        \ task to be solved’. Instead of learning from data, these AI systems learn from knowledge including rules, facts\
        \ and relationships encoded by human experts. Based on the human experts encoded knowledge, these systems can ‘reason’\
        \ via deductive or inductive engines or using operations such as sorting, searching, matching, chaining. By using\
        \ logical inference to draw conclusions, such systems apply formal logic, predefined rules or ontologies to new situations.\
        \ Logic- and knowledge-based approaches include for instance, knowledge representation, inductive (logic) programming,\
        \ knowledge bases, inference and deductive engines, (symbolic) reasoning, expert systems and search and optimisation\
        \ methods. \nFor detailed information, please refer to the [guidelines on the definition of an AI system, Section\
        \ 5 paragraphs 26-51.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-definition_en-1.pdf#nameddest=para_26)\n"
    - value: '4'
      label: The outputs generated by the AI system (e.g. predictions, recommendations or decisions) can influence physical
        or virtual environments
      help: "Following the European Commission's guidelines on the definition AI system, the ability of a system, to generate\
        \ outputs, such as predictions, content, and recommendations, based on inputs it receives and using machine learning\
        \ and logic and knowledge-based approaches, is fundamental to what AI systems do and what distinguishes those systems\
        \ from other forms of software. The capacity to generate outputs and the type of output the system can generate is\
        \ central to understanding the functionality and impact of an AI system.\nPredictions are one of the most common outputs\
        \ that AI system produce and that require the least human involvement. A prediction is an estimate about an unknown\
        \ value (the output) from known values supplied to the system (the input). Software systems have been used for decades\
        \ to generate predictions. AI systems using machine learning are capable of generating predictions that uncover complex\
        \ patterns in data and make accurate predictions in highly dynamic and complex environments.\nFor example, AI systems\
        \ deployed in self-driving cars are designed to make real-time predictions in an extremely complex and dynamic environment,\
        \ with multiple types of agents and interactions, and a practically infinite number of possible situations, and to\
        \ take decisions to adjust their behaviour accordingly. Non-AI systems, typically based on historical data, scientific\
        \ data or predefined rules, such as certain non-AI medical device expert systems, are not capable of dealing with\
        \ such a degree of complexity. Similarly, AI systems for energy consumption are designed to estimate energy consumption\
        \ by analysing data from smart meters, weather forecasts and behavioural patterns on consumers. By relying on machine\
        \ learning approaches, an AI system is designed to find complex correlations between these variables to make more\
        \ accurate energy consumption predictions.  \nContent refers to the generation of new material by an AI system. This\
        \ may include text, images, videos, music and other forms of output. There is an increasing number of AI systems that\
        \ use machine learning models (for example based on Generative Pre-trained Transformer (GPT) technologies) to generate\
        \ content. Although content, as a category of output, may be understood from a technical perspective in terms of a\
        \ sequence of ‘predictions’ or ‘decisions’, due to the prevalence of this output in generative AI systems, it is listed\
        \ in Recital 12 AI Act as a separate category of output. \nRecommendations refer to suggestions for specific actions,\
        \ products, or services to users based on their preferences, behaviours, or other data inputs. Similarly to predictions,\
        \ both AI-based and non-AI-based systems can be designed to generate recommendations. AI-based recommendation systems,\
        \ for example, can leverage large-scale data, adapt to user behaviour in real-time, provide highly personalised recommendations,\
        \ and scale efficiently as the dataset grows, the functionalities that non-AI systems that rely on static, rule-based\
        \ mechanisms and limited data, rarely possess. In other cases, recommendations refer to potential decisions, such\
        \ as a candidate to hire in a recruitment system, which will be evaluated by humans. If these recommendations are\
        \ automatically applied, they become decisions.   \nDecisions refer to conclusions or choices made by a system. An\
        \ AI system that outputs a decision automates processes that are traditionally handled by human judgement. Such a\
        \ system implies a fully automated process whereby a certain outcome is produced in the environment surrounding the\
        \ system without any human intervention.    \nIn summary, AI systems, including systems based on machine learning\
        \ approaches and logic or knowledge-based systems, differ from non-AI systems in their ability to generate outputs\
        \ like predictions, content, recommendation, and decisions in that they can handle complex relationships and patterns\
        \ in data. AI systems can generally generate more nuanced outputs than other systems, for example, by leveraging patterns\
        \ learned during training or by using expert-defined rules to make decisions, offering more sophisticated reasoning\
        \ in structured environments. For more detailed information, please refer to the [guidelines on the definition of\
        \ an AI system, Section 6, paragraphs 52-59.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-definition_en-1.pdf#nameddest=para_52)\
        \ \nReference to ‘physical or virtual environments’ indicates that the influence of an AI system may be both to tangible,\
        \ physical objects (e.g. robot arm) and to virtual environments, including digital spaces, data flows, and software\
        \ ecosystems. For more detailed information, please refer to the [guidelines on the definition of an AI system, Section\
        \ 7, paragraphs 60-61.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-definition_en-1.pdf#nameddest=para_60)"
    - value: '5'
      label: None of the above
      help: "Following the European Commission's guidelines on the definition of an AI system, Recital 12 explains that the\
        \ AI system definition should distinguish AI systems from 'simpler traditional software systems or programming approaches\
        \ and should not cover systems that are based on the rules defined solely by natural persons to automatically execute\
        \ operations.'\nSome systems have the capacity to infer in a narrow manner but may nevertheless fall outside of the\
        \ scope of the AI system definition because of their limited capacity to analyse patterns and adjust autonomously\
        \ their output. Such systems may include: \n- Systems for improving mathematical optimization:  systems used to improve\
        \ mathematical optimisation or to accelerate and approximate traditional, well established optimization methods, such\
        \ as linear or logistic regression methods, fall outside the scope of the AI system definition. This is because, while\
        \ those models have the capacity to infer, they do not transcend ‘basic data processing’. An indication that a system\
        \ does not transcend basic data processing could be that it has been used in consolidated manner for many years. This\
        \ includes, for example, machine learning-based models that approximate functions or parameters in optimization problems\
        \ while maintaining performance. The systems aim to improve the efficiency of optimization algorithms used in computational\
        \ problems. For example, they help to speed up optimisation tasks by providing learned approximations, heuristics,\
        \ or search strategies. \n- Basic data processing: basic data processing system refers to a system that follows predefined,\
        \ explicit instructions or operations. These systems are developed and deployed to execute tasks based on manual inputs\
        \ or rules, without any ‘learning, reasoning or modelling’ at any stage of the system lifecycle. They operate based\
        \ on fixed human-programmed rules, without using AI techniques, such as machine learning or logic-based inference,\
        \ to generate outputs. These basic data processing systems include, for example, database management systems used\
        \ to sort or filter data based on specific criteria (e.g. ‘find all customers who purchased a specific product in\
        \ the last month’), standard spreadsheet software applications which do not incorporate AI enabled functionalities,\
        \ and software that calculates a population average from a survey that is later exploited in a general context. \n\
        - Systems based on classical heuristics: classical heuristics are problem-solving techniques that rely on experience-based\
        \ methods to find approximate solutions efficiently. Heuristics techniques are commonly used in programming situations\
        \ where finding an exact solution is impractical due to time or resource constraints. Classical heuristics typically\
        \ involve rule-based approaches, pattern recognition, or trial-and-error strategies rather than data-driven learning.\
        \ Unlike modern machine learning systems, which adjust their models based on input-output relationships, classical\
        \ heuristic systems apply predefined rules or algorithms to derive solutions. For instance, a chess program using\
        \ a minimax algorithm with heuristic evaluation functions can assess board positions without requiring prior learning\
        \ from data. While effective in many applications, heuristic methods may lack adaptability and generalization compared\
        \ to AI systems that learn from experience. \n- Simple prediction systems: all machine-based systems whose performance\
        \ can be achieved via a basic statistical learning rule, while technically may be classified as relying on machine\
        \ learning approaches fall outside the scope of the AI system definition, due to its performance. For instance, in\
        \ financial forecasting (basic benchmarking) such machine-based systems may be used to predict future stock prices\
        \ by using an estimator with the ’mean‘ strategy to establish a baseline prediction (e.g., always predicting the historical\
        \ average price). Another example is using the average temperature of last week for predicting tomorrow’s temperature.\
        \ This baseline system solely estimates averages, but it is not achieving the performance of more complex time-series\
        \ forecasting  systems that would require more sophisticated models. For detailed information, please refer to the\
        \ [guidelines on the definition of an AI system, Section 5.2, paragraphs 40-51.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-definition_en-1.pdf#nameddest=para_40)\n\
        \n"
  QAIS 1:
    id: QAIS 1
    question: Is your system an AI system within the meaning of the AI Act?
    info: "The AI Act defines AI systems as 'machine-based system that is designed to operate with varying levels of autonomy\
      \ and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the\
      \ input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence\
      \ physical or virtual environments' (Article 3 (1), Recital 12).\n \nThe AI system definition should distinguish AI\
      \ systems from 'simpler traditional software systems or programming approaches and should not cover systems that are\
      \ based on the rules defined solely by natural persons to automatically execute operations' (Recital 12)."
    type: single_choice
    sources: Article 3
    options:
    - value: '0'
      label: 'Yes'
      help: ''
    - value: '1'
      label: 'No'
      help: ''
    - value: '2'
      label: Uncertain
      help: ''
  QAIS 2.1:
    id: QAIS 2.1
    question: As a downstream distributor, importer, deployer, or third-party, have you made any of the following modifications
      to an AI system?
    info: ''
    type: multiple_choice
    sources: Article 25; Recital 84; Recital 87; Recital 88; Recital 89
    options:
    - value: '0'
      label: You put your name or trademark on the AI system
      help: ''
    - value: '1'
      label: You made a substantial modification to the AI system that has already been placed on the market or put into service
      help: "Under the AI Act, substantial modification is defined as 'a change to an AI system after its placing on the market\
        \ or putting into service which is not foreseen or planned in the initial conformity assessment carried out by the\
        \ provider and as a result of which the compliance of the AI system' with some requirements set out in the AI Act\
        \ is affected or results in a modification to the intended purpose for which the AI system has been assessed (Article\
        \ 3 (23)).\n \nSubstantial modification should be understood as 'whenever a change occurs which may affect the compliance\
        \ of a high-risk AI system with the AI Act (e.g. change of operating system or software architecture), or when the\
        \ intended purpose of the system changes, that AI system should be considered to be a new AI system which should undergo\
        \ a new conformity assessment. However, changes occurring to the algorithm and the performance of AI systems which\
        \ continue to 'learn' after being placed on the market or put into service, namely automatically adapting how functions\
        \ are carried out, should not constitute a substantial modification, provided that those changes have been pre-determined\
        \ by the provider and assessed at the moment of the conformity assessment' (Recital 128)."
    - value: '2'
      label: You modified the intended purpose of the AI system in such a way that the AI system becomes a high-risk AI system
      help: ''
    - value: '3'
      label: None of the above
      help: ''
  QAIS 2.2:
    id: QAIS 2.2
    question: 'Does your product integrate an AI system or is itself an AI system, and do you currently fall under any of
      the following scenarios:'
    info: ''
    type: single_choice
    sources: Article 25; Recital 87
    options:
    - value: '0'
      label: The AI system is placed on the market together with the product under your name or trademark?
      help: ''
    - value: '1'
      label: The AI system is put into service under your name or trademark after the product has been placed on the market?
      help: ''
    - value: '2'
      label: None of the above
      help: ''
  QAIS 2:
    id: QAIS 2
    question: 'Are you or your organisation responsible for any of the following roles within the EU? Note: If you have multiple
      roles, such as provider and deployer, please complete the assessment separately for each role to ensure a comprehensive
      evaluation'
    info: ''
    type: multiple_choice
    sources: Article 2 (1); Article 3; Recital 87
    options:
    - value: '0'
      label: Provider
      help: A 'provider' means a natural or legal person, public authority, agency or other body that develops an AI system
        or a general-purpose AI model or that has an AI system or a general-purpose AI model developed and places it on the
        market or puts the AI system into service under its own name or trademark, whether for payment or free of charge (Article
        3 (3)).
    - value: '1'
      label: Deployer
      help: A 'deployer' means a natural or legal person, public authority, agency or other body using an AI system under
        its authority except where the AI system is used in the course of a personal non-professional activity (Article 3
        (4)).
    - value: '2'
      label: Importer
      help: An 'importer' means a natural or legal person located or established in the Union that places on the market an
        AI system that bears the name or trademark of a natural or legal person established in a third country (Article 3
        (6)).
    - value: '3'
      label: Distributor
      help: A 'distributor' means a natural or legal person in the supply chain, other than the provider or the importer,
        that makes an AI system available on the Union market (Article 3 (7)).
    - value: '4'
      label: Authorised representative
      help: An 'authorised representative' means a natural or legal person located or established in the Union who has received
        and accepted a written mandate from a provider of an AI system or a general-purpose AI model to, respectively, perform
        and carry out on its behalf the obligations and procedures established by the AI Act (Article 3 (5)).
    - value: '5'
      label: Product manufacturer
      help: A 'product manufacturer' is an operator that places an AI system on the market or puts it into service alongside
        its product, under their own name or trademark (Article 2 (1) e)).
    - value: '6'
      label: You are a natural person using an AI system for purely personal non-professional activity
      help: ''
  QAIS 3:
    id: QAIS 3
    question: Do you meet any of the following statement?
    info: '''In light of their digital nature, certain AI systems should fall within the scope of the AI Act even when they
      are not placed on the market, put into service, or used in the Union'' (Recital 22)'
    type: multiple_choice
    sources: Article 2; Recital 21; Recital 22
    options:
    - value: '0'
      label: Your AI system is placed or put in service within the EU
      help: ''
    - value: '1'
      label: You are established or located within the EU
      help: ''
    - value: '2'
      label: Your AI system's output is used in the EU
      help: ''
    - value: '3'
      label: An AI system is integrated with your product under your own name or trademark
      help: ''
    - value: '4'
      label: None of the above
      help: ''
  QAIS 4:
    id: QAIS 4
    question: Does your AI system meet any of the following descriptions?
    info: ''
    type: multiple_choice
    sources: Article 2; Recital 22; Recital 23; Recital 25
    options:
    - value: '0'
      label: The AI system is exclusively developed and used for military, defence or national security purposes
      help: ''
    - value: '1'
      label: The AI system is used by public authorities in third countries or international organisations in the framework
        of international cooperation for law enforcement and judicial cooperation
      help: ''
    - value: '2'
      label: The AI system is specifically developed and put into service for the sole purpose of scientific research and
        development
      help: Testing in real world conditions shall not be covered by that exclusion.
    - value: '3'
      label: The AI system is used in research, testing or development activity prior to being placed on the market or put
        into service
      help: ''
    - value: '4'
      label: None of the above
      help: ''
  QAIS 5:
    id: QAIS 5
    question: Does your AI system fall under one of these AI practices?
    info: ''
    type: multiple_choice
    sources: Article 3; Article 5; Recital 15; Recital 16; Recital 18; Recital 29; Recital 31; Recital 42; Recital 43; Recital
      44; [Guidelines on prohibited AI practices](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_44)
    options:
    - value: '0'
      label: AI system deploys subliminal, manipulative or deceptive techniques to materially distort a person's behaviour,
        thereby likely causing significant harm
      help: "Following the European Commission’s guidelines on prohibited AI practices: \nSubliminal, manipulative or deceptive\
        \ techniques include 'in particular unfair commercial practices leading to economic or financial harms to consumers\
        \ are prohibited under all circumstances, irrespective of whether they are put in place through AI systems or otherwise'\
        \ (Recital 29).\nThe AI Act does not prohibit 'lawful practices in the context of medical treatment such as psychological\
        \ treatment of a mental disease or physical rehabilitation, when those practices are carried out in accordance with\
        \ the applicable law and medical standards, for example explicit consent of the individuals or their legal representatives.\
        \ In addition, common and legitimate commercial practices, for example in the field of advertising, that comply with\
        \ the applicable law should not, in themselves, be regarded as constituting harmful manipulative AI-enabled practices'\
        \ (Recital 29).\nIt should be noted that it is 'not possible to assume that there is an intention to distort behaviour\
        \ where the distortion results from factors external to the AI system which are outside the control of the provider\
        \ or the deployer, namely factors that may not be reasonably foreseeable and therefore not possible for the provider\
        \ or the deployer of the AI system to mitigate. In any case, it is not necessary for the provider or the deployer\
        \ to have the intention to cause significant harm, provided that such harm results from the manipulative or exploitative\
        \ AI-enabled practices' (Recital 29).\nMain components of the prohibitions in Article 5(1)(a) AI Act: harmful manipulation\n\
        1.\tSubliminal, purposefully manipulative or deceptive techniques:\n\n-\tSubliminal techniques:\nThe Commission guidelines\
        \ on prohibited practices further detail that the subliminal techniques must be capable of influencing behaviour in\
        \ ways in which the person remains unaware of such influence, how it works, or its effects on the person’s decision-making\
        \ or value- and opinion-formation. In particular, subliminal techniques may use stimuli delivered through audio, visual,\
        \ or tactile media that are too brief or subtle to be noticed and that have been traditionally known and prohibited\
        \ in other sectors, such as media advertising. These stimuli, while not consciously perceived, may still be processed\
        \ by the brain and influence behaviour. For detailed information, please refer to the [guidelines on prohibited AI\
        \ practices, Section 3.2.1, paragraphs 63-66.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_63)\n\
        -\tPurposefully manipulative techniques and deceptive techniques\nManipulative or deceptive techniques are referred\
        \ as 'techniques that subvert or impair person's autonomy, decision-making or free choice in ways that people are\
        \ not consciously aware of those techniques or, where they are aware of them, can still be deceived or are not able\
        \ to control or resist them. This could be facilitated, for example, by machine-brain interfaces or virtual reality\
        \ as they allow for a higher degree of control of what stimuli are presented to persons, insofar as they may materially\
        \ distort their behaviour in a significantly harmful manner.' (Recital 29)\na)\tPurposefully manipulative techniques:\n\
        Following the Commission guidelines on prohibited AI practices. While not all manipulative techniques operate beyond\
        \ the threshold of conscious awareness, many do and there may be an overlap with subliminal techniques, since such\
        \ techniques also ultimately have manipulative effects. Recital 29 AI Act clarifies that the prohibition in Article\
        \ 5(1)(a) also covers techniques where individuals, even if they are aware of the influence attempt, may not be able\
        \ to control or resist its manipulative effect. As a result, individuals are influenced or pushed into behaviour and\
        \ decisions they would normally not have made if they were not subjected to the manipulative techniques to a point\
        \ that undermines their individual autonomy or free choice. The prohibition against purposefully manipulative techniques\
        \ also covers AI systems that manipulate individuals without any human intending them to do so. For detailed information,\
        \ please refer to the [guidelines on prohibited AI practices, Section 3.2.1, paragraphs 67-69.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_67)\n\
        b)\tDeceptive techniques:\nFollowing the guidelines on prohibited AI practices: ‘deceptive techniques’ deployed by\
        \ AI systems should be understood to involve presenting false or misleading information with the objective or the\
        \ effect of deceiving individuals and influencing their behaviour in a manner that undermines their autonomy, decision-making\
        \ and free choices.\nIn this context, the interplay between the prohibition in Article 5(1)(a) AI Act and the deployer’s\
        \ obligations in Article 50(4) AI Act to label ‘deep fakes’ and certain AI-generated text publications on matters\
        \ of public interest, as well as the provider’s obligation to ensure AI systems interacting with people are designed\
        \ in a way to inform people that they are interacting with AI and not a human, should be clarified. Such visible disclosure\
        \ constitutes a mitigating measure that should also be enabled through design features embedded in the AI system provided\
        \ by the provider, including technical measures enabling the detection of AI-generated and manipulated content. The\
        \ visible labelling of ‘deep fakes’ and chatbots reduces the risk of deception that is likely to arise once the AI-generated\
        \ content is disseminated to the public and reduces the risk of harmful distorting effects on the individual’s opinion-\
        \ and belief-formation and behaviour.\nBy contrast, the prohibition in Article 5(1)(a) AI Act has a much more limited\
        \ scope. It may, for example, cover cases where a chatbot or deceptive AI-generated content presents false or misleading\
        \ information in ways that aim to or have the effect of deceiving individuals and distorting their behaviour that\
        \ would not have happened if they were not exposed to the interaction with the AI system or the deceptive AI generated\
        \ content, in particular if this has not been visibly disclosed. For detailed information, please refer to the [guidelines\
        \ on prohibited AI practices, Section 3.2.1, paragraphs 70-73.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_70)\n\
        For detailed information about subliminal, manipulative and deceptive techniques, please refer to the [guidelines\
        \ on prohibited AI practices, Section 3.2, paragraphs 60-75.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_60)\n\
        \n2.\tWith the objective or the effect of materially distorting the behaviour of a person or a group of persons\n\
        One of the conditions for the prohibition in Article 5(1)(a) AI Act to apply is that the deployed subliminal, purposefully\
        \ manipulative, or deceptive technique must have ‘the objective, or the effect of materially distorting the behaviour\
        \ of a person or a group of persons’. Following the guidelines on prohibited AI practices, this implies a substantial\
        \ impact on the behaviour where a person’s autonomy and free choices are undermined, rather than a minor influence.\
        \ However, intent is not a necessary requirement, since Article 5(1)(a) AI Act also covers practices that may only\
        \ have the ‘effect’ of causing material distortion. There should be a plausible/reasonably likely causal link between\
        \ the potential material distortion of the behaviour and the subliminal, purposefully manipulative or deceptive technique\
        \ deployed by the AI system.\nThe concept of ‘material distortion of the behaviour’ of a person or a group of persons\
        \ is central to Article 5(1)(a) AI Act. It involves the deployment of subliminal, purposefully manipulative or deceptive\
        \ techniques that are capable of influencing people’s behaviour in a manner that appreciably impairs their ability\
        \ to make an informed decision, thereby causing them to behave in a way or to take a decision that they would otherwise\
        \ not have taken.  ‘Appreciable impairment’ refers to a substantially reduced ability to make informed and autonomous\
        \ decisions, thereby causing individuals to behave in a way or to take a decision that they would otherwise not have\
        \ taken. It goes beyond minor or negligible impacts and involves a significant distortion or hindrance in decision-making\
        \ and free choice, including in relation to opinion- and belief-formation. This suggests that ‘material distortion’\
        \ involves a degree of coercion, manipulation, or deception that goes beyond lawful persuasion, which falls outside\
        \ the scope of the prohibition.\nAn ‘informed decision’ requires an understanding and knowledge of the relevant information,\
        \ including the available options, the risks and benefits of each choice, the possible effects of the AI system on\
        \ their behaviour, and, as appropriate, other contextual information that is important for the decision-making or\
        \ the behaviour of the person.\nFor detailed information about subliminal, manipulative and deceptive techniques,\
        \ please refer to the [guidelines on prohibited AI practices, Section 3.2.2, paragraphs 76-85.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_76)\n\
        3.\t(Reasonably likely to) cause significant harm\nFollowing the Commission guidelines on prohibited AI practices,\
        \ the distortion of the behaviour of a person or group of persons must cause or be reasonably likely to cause that\
        \ person, another person, or group of persons significant harm. \na)\tTypes of harms\nThe AI Act addresses various\
        \ types of harmful effects associated with manipulative and deceptive AI systems, each with distinct implications\
        \ for individual persons and groups of persons that may be affected. The main types of harms relevant for Article\
        \ 5(1)(a) AI Act include physical, psychological, financial, and economic harms that may be compound with broader\
        \ societal harms in certain cases. For detailed information, please refer to the [guidelines, section 3.2.3, paragraphs\
        \ 86-90.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_86)\n\
        b)\tThreshold for significance of the harm\nThe prohibition in Article 5(1)(a) AI Act only applies if the harm caused\
        \ by the subliminal, manipulative and deceptive techniques is ‘significant’. The AI Act does not provide a definition\
        \ for the concept of ‘significant harm’, but it should be understood as implying significant adverse impacts on physical,\
        \ psychological health or financial and economic interests of persons and groups of persons. The determination of\
        \ ‘significant harm’ is fact -specific, requiring careful consideration of each case’s individual circumstances and\
        \ a case-by-case assessment, but the individual effects should be always material and significant in each case. For\
        \ detailed information, please refer to the [guidelines, section 3.2.3, paragraphs 91-93 and 134.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_91)\n\
        c)\tCausal link and threshold for reasonable likelihood of the harm\nThe concept of ‘reasonably likely’ is used in\
        \ Article 5(1)(a) AI Act to determine whether there is a plausible/reasonably likely causal link between the manipulative\
        \ or deceptive technique capable of distorting the person’s behaviour in a manner that undermines their free choices\
        \ and the potential significant harm. This concept allows the application of the prohibition not only in cases where\
        \ the harm has occurred, but also where it is reasonably likely to occur in line with the safety logic of the AI Act.\
        \ In this context, it is particularly relevant to assess whether the provider or deployer of the AI system could have\
        \ reasonably foreseen the significant harm that is reasonably likely to result from the subliminal, purposefully manipulative\
        \ or the deceptive techniques deployed and whether they implemented appropriate preventive and mitigating measures\
        \ to avoid or mitigate the risk of such significant harms. This implies a judgement of reasonableness on an objective\
        \ basis and according to universally accepted criteria (e.g. technical and scientific), including a criterion of rationality\
        \ in establishing plausible causality between the AI practice and the significant harm that may arise. The opacity\
        \ or transparency of the AI system and its functioning may affect the conclusion regarding this causal link and, hence,\
        \ the application of the prohibition. For detailed information, please refer to the [guidelines, section 3.2.3, paragraphs\
        \ 94-96.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_94)\
        \ \nFor detailed information about harmful manipulation and deception, please refer to the [guidelines on prohibited\
        \ AI practices, section 3, paragraphs 58-97.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_58)\n\
        \n"
    - value: '1'
      label: AI system exploits of vulnerabilities due to age, disability or a specific socioeconomic situation to materially
        distort a person's behaviour, thereby likely causing significant harm
      help: "Following the European Commission’s guidelines on prohibited AI practices:\n1)\tExploitation of vulnerabilities\
        \ due to age, disability, or a specific socio-economic situation\nThe guidelines on the prohibited AI practices further\
        \ detail that the concept of ‘vulnerabilities’ may be understood to encompass a broad spectrum of categories, including\
        \ cognitive, emotional, physical, and other forms of susceptibility that can affect the ability of an individual or\
        \ a group of persons to make informed decisions or otherwise influence their behaviour. While Article 5(1)(b) AI Act\
        \ refers to ‘any’ vulnerability, it limits the relevant persons covered by the prohibition to those defined by their\
        \ age, disability, or socio-economic situations, who in principle have more limited capacity to recognise or resist\
        \ the AI manipulative or exploitative practices and are in need of enhanced protection. It follows from the wording\
        \ of Article 5(1)(b) AI Act that this susceptibility must be the result of the person belonging to one of the groups\
        \ (‘due to’).\n‘Exploitation’ should be understood as objectively making use of such vulnerabilities in a manner that\
        \ is harmful for the exploited (groups of) persons or other persons and should be clearly distinguished from lawful\
        \ practices that are not affected by the prohibition. Exploitation of the vulnerabilities of persons belonging to\
        \ those clearly defined groups may be cumulative (reference to ‘any’) which in combination may also constitute an\
        \ aggravating factor that is likely to increase the harm. Exploitation of vulnerabilities of persons and groups of\
        \ persons belonging to vulnerable groups other than those defined by age, disability or specific socio-economic situation\
        \ are outside the scope of Article 5(1)(b) AI Act.\nA)\tAge \nAge is a primary vulnerability category covered by the\
        \ prohibition in Article 5(1)(b) AI Act, including both young and older people. That prohibition aims to prevent AI\
        \ systems from exploiting cognitive and other limitations that children and older people may have, and to protect\
        \ them from harmful undue influence, manipulation and exploitation. For detailed information, please refer to the\
        \ [guidelines on the prohibited AI practices, section 3.3.1 paragraphs 104-106.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_104)\n\
        B)\tDisability\nDisability encompasses a wide range of long-term physical, mental, intellectual, and sensory impairments\
        \ which in interaction with other barriers hinder full and effective participation of individuals in the society on\
        \ an equal basis with others. AI systems that exploit such vulnerabilities may be particularly harmful for persons\
        \ with disabilities which can be more easily influenced or exploited due to their impairment compared to other persons.\
        \ For detailed information, please refer to the [guidelines on the prohibited AI practices, Section 3.3.1 paragraphs\
        \ 107-108.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_107)\n\
        C)\tSpecific socio-economic situation\n‘Specific’ should not be interpreted in this context as a unique individual\
        \ characteristic, but rather a legal status or membership to a specific vulnerable social or economic group. Recital\
        \ 29 AI Act contains a non-exhaustive list of examples of such situations, such as persons living in extreme poverty\
        \ and ethnic or religious minorities. The category aims to cover, in principle, relatively stable and long-term characteristics,\
        \ but transient circumstances, such as temporary unemployment, over-indebtedness or migration status, may also be\
        \ covered as a specific socio-economic situation. However, situations such as grievances or loneliness that may be\
        \ experienced by any person are not covered, since they are not specific from a socio-economic perspective (their\
        \ exploitation may be covered though under Article 5(1)(a) AI Act). For detailed information, please refer to the\
        \ [guidelines on the prohibited AI practices, Section 3.3.1, paragraphs 109-112.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_109)\n\
        2)\tWith the objective or the effect of materially distorting\nThe European Commission's guidelines on prohibited\
        \ AI practices further detail that the objective or the effect of materially distorting behaviour implies a substantial\
        \ impact, rather than a minor or trivial one, but does not necessarily require intent, since Article 5(1)(b) AI Act\
        \ covers practices that may only have the ‘effect’ of causing material distortions. Article 5(1)(a) and (b) AI Act\
        \ make use of the same concepts and should therefore be interpreted in the same way. The only noteworthy difference\
        \ is the need in Article 5(1)(a) AI Act for the exploitative practice to ‘appreciably impair the ability to make an\
        \ informed decision’, which is not present in Article 5(1)(b) AI Act, since the specific vulnerabilities of children\
        \ and other vulnerable persons reduce their capacity to make such informed decisions and force them into adopting\
        \ behaviour against which they cannot protect themselves as other adults might do.\n3)\t(Reasonably likely to) cause\
        \ significant harm \nSignificant harm encompasses a range of significant adverse impacts, including physical, psychological,\
        \ financial, and economic harms that must be reasonably likely to occur for the prohibition in Article 5(1)(b) AI\
        \ Act to apply. For vulnerable groups — children, older persons, persons with disabilities, and socio economically\
        \ disadvantaged populations — these harms may be particularly severe and multifaceted due to their heightened susceptibility\
        \ to exploitation. What may be considered an acceptable risk of harm for adults often represents an unacceptable harm\
        \ for children and these other vulnerable groups. A precautionary approach is therefore particularly warranted in\
        \ case of uncertainty and potential for significant harms. For detailed information, please refer to the [guidelines\
        \ on the prohibited AI practices, section 3.3.3, paragraphs 114-121 and 134.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_114)\n\
        For details about the interplay between the prohibitions in Article 5(1)a) and (b) AI Act, please refer to the [guidelines\
        \ on prohibited practices, Section 3.4, paragraphs 122-125.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_122)\n"
    - value: '2'
      label: AI system carries out social scoring leading to detrimental or unfavorable treatment of certain natural persons
        or groups of persons
      help: "The AI Act refers to social scoring as 'AI systems (that) evaluate or classify natural persons or groups thereof\
        \ on the basis of multiple data points related to their social behaviour in multiple contexts or known, inferred or\
        \ predicted personal or personality characteristics over certain periods of time' (Recital 31).\n\nHowever, the prohibition\
        \ 'should not affect lawful evaluation practices of natural persons that are carried out for a specific purpose in\
        \ accordance with Union and national law' (Recital 31).\n\nFollowing the European Commission’s guidelines on prohibited\
        \ AI practices: \n\n1)\t‘Social scoring’: evaluation or classification based on social behaviour or personal or personality\
        \ characteristics over a certain period of time\na)\tEvaluation or classification of natural persons or group of persons\n\
        The score produced by the system may take various forms, such as a mathematical number (for example, from 0 to 1),\
        \ a ranking, or a label.\nThe scope of the prohibition is broad covering evaluation and classification practices in\
        \ both the public and the private sector. At the same time the evaluation or classification concerns only natural\
        \ persons or groups of natural persons, thereby excluding in principle legal entities. \nWhile ‘evaluation’ suggests\
        \ the involvement of some form of an assessment or judgement about a person or group of persons, a simple classification\
        \ of persons or groups of persons based on characteristics, such as their age, sex, and height, need not necessarily\
        \ lead to evaluation. The scope of ‘classification’ is therefore broader than ‘evaluation’ and can also cover other\
        \ types of classifications or categorisations of natural persons or groups of persons based on criteria that do not\
        \ necessarily involve a particular assessment or judgement about those persons or groups of persons and their characteristics\
        \ or behaviour.\nThe term ‘evaluation’ also relates to the concept of ‘profiling’. Profiling means the use of information\
        \ about an individual (or group of individuals) and evaluating their characteristics or behaviour patterns in order\
        \ to place them into a certain category or group, in particular to analyse and/or make predictions about, for example,\
        \ their ability to perform a task; interests; or likely behaviour’. For detailed information, please refer to the\
        \ [guidelines on prohibited AI practices, Section 4.2.1, paragraphs 151-154.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_151)\n\
        b)\tOver a certain period of time\nThis suggests that the assessment should not be limited to a one-time or an at\
        \ once rating or grading with data or behaviour from a very specific individual context. At the same time, it is important\
        \ that this condition is assessed, taking all circumstances of the case into account to avoid circumvention of the\
        \ scope of the prohibition.\nc)\tBased on their social behaviour or known, inferred or predicted personal or personality\
        \ characteristics \n‘Social behaviour’ is a broad term that can generally include actions, behaviour, habits, interactions\
        \ within society, etc., and usually covers behaviour related data points from multiple sources. This could include\
        \ behaviour of individuals and groups of individuals in social and private contexts, such as participation in cultural\
        \ events, volunteering, etc., but also social behaviour in business contexts, for example the payment of debts, behaviour\
        \ when using certain services, as well as relations with public and private entities, government, police, and the\
        \ law (for example, whether a person obeys traffic rules). Social behaviour data from multiple contexts and data points\
        \ may be collected in a centralised way by the same entity, but is most often collected in a distributed way and combined\
        \ from different sources, which may involve increased monitoring and the tracking of individuals (so called ‘dataveillance’).\n\
        ‘Personal characteristics’ may include a variety of information relating to a person, for example sex, sexual orientation\
        \ or sexual characteristics, gender, gender identity, race, ethnicity, family situation, address, income, household\
        \ members, profession, employment or other legal status, performance at work, economic situation, financial liquidity,\
        \ health, personal preferences, interests, reliability, behaviour, location or movement, level of debt, type of car\
        \ etc. \nFor detailed information, please refer to the [guidelines on prohibited AI practices, Section 4.2.1, paragraphs\
        \ 156-159.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_156)\n\
        \n2)\tThe social score must lead to detrimental or unfavourable treatment in unrelated social contexts and/or unjustified\
        \ or disproportionate treatment to the gravity of the social behaviour\nFor the prohibition in Article 5(1)(c) AI\
        \ Act to apply, the social score created by or with the assistance of an AI system must lead to a detrimental or unfavourable\
        \ treatment for the evaluated person or group of persons. In other words, the treatment must be the consequence of\
        \ the score, and the score the cause of the treatment. Such a plausible causal link may also exist in cases where\
        \ the harmful consequences have not yet materialised but the AI system is intended to or capable of producing such\
        \ an adverse outcome.\nThe final condition for the prohibition in Article 5(1)(c) AI Act to apply is that the use\
        \ of the social score must result (or be capable of resulting) in detrimental or unfavourable treatment either: \n\
        i.\tin social context(s) unrelated to the contexts in which the data was originally generated or collected, or \n\
        ii.\tunjustified or disproportionate to the social behaviour or its gravity.\nThese conditions are alternative and\
        \ may apply also in combination. An analysis on a case-by-case basis is necessary to assess if at least one of them\
        \ is fulfilled, since many AI-enabled scoring and evaluation practices may not fulfil them and therefore be outside\
        \ the scope of the prohibition.\nFor detailed information, please refer to the [guidelines on prohibited AI practices,\
        \ Section 4.2.2, paragraphs 160-169.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_160)\n\
        \n3)\tOut of scope\nFollowing the guidelines on prohibited AI practices, the prohibition in Article 5(1)(c) AI Act\
        \ only applies to the scoring of natural persons or groups of persons, thus excluding in principle scoring of legal\
        \ entities where the evaluation is not based on personal or personality characteristics or social behaviour of individuals,\
        \ even if in some cases individuals may be indirectly impacted by the score (e.g. all citizens in a municipality in\
        \ case of allocation of budget). However, if legal entities have been evaluated based on an overall score that aggregates\
        \ the evaluation or classification of a group of natural persons based on their social behaviour or personal or personality\
        \ characteristics and this score directly affects those persons (e.g., all employees in a company, students in a specific\
        \ school whose behaviour has been evaluated), the practice may fall within the scope of Article 5(1)(c) AI Act if\
        \ all other conditions are fulfilled. This will depend on a case-by-case assessment.\nAI-based social scoring as a\
        \ ‘probabilistic value’ and prognosis should also be distinguished from individual ratings by users which assess the\
        \ quality of a service (such as a driver in an online car-sharing platform or a host in an online platform for accommodation).\n\
        For detailed information, please refer to the [guidelines on prohibited AI practices, Section 4.3, paragraphs 173-177.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_173)\n"
    - value: '3'
      label: AI system does individual criminal offence risk assessment and prediction for natural persons solely on the profiling
        or assessment of personality traits or characteristics
      help: "Natural persons should never be judged on AI-predicted behaviour based solely on their profiling, personality\
        \ traits or characteristics, such as nationality, place of birth, place of residence, number of children, level of\
        \ debt or type of car, without a reasonable suspicion of that person being involved in a criminal activity based on\
        \ objective verifiable facts and without human assessment thereof. Therefore, risk assessments carried out with regard\
        \ to natural persons in order to assess the likelihood of their offending or to predict the occurrence of an actual\
        \ or potential criminal offence based solely on profiling them or on assessing their personality traits and characteristics\
        \ should be prohibited.' (Recital 42)\nHowever, the prohibition 'does not refer to or touch upon risk analytics that\
        \ are not based on the profiling of individuals or on the personality traits and characteristics of individuals, such\
        \ as AI systems using risk analytics' (Recital 42). \n\nFollowing the European Commission’s guidelines on prohibited\
        \ AI practices: \n1)\tAssessing the risk or predicting the likelihood of a person committing a crime\nRisk assessments\
        \ and predictions are, in principle, forward-looking and concern future criminal offences (not yet committed) or crimes\
        \ that are assessed as a risk of being committed at the moment, including in cases of an attempt or preparatory activities\
        \ undertaken to commit a criminal offence. They can be made at any stage of the law enforcement activities. For detailed\
        \ information, please refer to the  [guidelines on prohibited AI practices, Section 5.2.1, paragraphs 189-192.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_189)\n\
        2)\tSolely based on profiling of a natural person or on assessing their personality traits and characteristics\na)\t\
        Profiling of a natural person\nArticle 3(52) AI Act defines ‘profiling’ by reference to its definition in Article\
        \ 4(4) GDPR. The concept of profiling includes the objective to ‘evaluate certain personal aspects’ as one of its\
        \ core elements. In the context of Article 5(1)(d) AI Act, the profiling is done for the purposes of assessing or\
        \ predicting the risk of a person committing a crime.\nb)\tAssessment of personality traits and characteristics\n\
        The prohibition also applies if the risk assessment to assess or predict the risk of the person committing a criminal\
        \ offence is only based on assessing the person’s personality traits and characteristics.\nc)\tSolely\nThe condition\
        \ that the risk assessment must be based ‘solely’ on profiling or assessing personality traits and characteristics\
        \ may not be fulfilled in a number of situations. As is evident from the last phrase of Article 5(1)(d) AI Act, such\
        \ a situation arises, in any event, where the AI system is used to support the human assessment of the involvement\
        \ of a person in a criminal activity, which is already based on objective and verifiable facts directly linked to\
        \ a criminal activity.\nHowever, there can also be other situations, which will always need to be assessed on a case-by-case\
        \ basis. On the one hand, the use of the term ‘solely’ leaves open the possibility of various other elements being\
        \ taken into account in the risk assessment, which makes that it is no longer based on profiling or assessing personality\
        \ traits or characteristics alone. On the other hand, in order to avoid circumvention of the prohibition and ensure\
        \ its effectiveness, any such other elements will have to be real, substantial and meaningful for them to be able\
        \ to justify the conclusion that the prohibition does not apply. A reading of the prohibition of Article 5(1)(d) AI\
        \ Act together with the exclusion contained in the last phrase thereof suggests that, in particular, the existence\
        \ of certain pre-established objective and verifiable facts may justify that conclusion.\nFor detailed information\
        \ about point 2, please refer to the [guidelines on prohibited AI practices, Section 5.2.2, paragraphs 193-202.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_193)\n\
        \n3)\tExclusion of AI systems to support the human assessment based on objective and verifiable facts directly linked\
        \ to a criminal activity\nWhere the system falls within the scope of the exclusion and is therefore not prohibited,\
        \ it will be classified as a high-risk AI system (as referred to in Annex III, point 6(d), AI Act) if intended to\
        \ be used by law enforcement authorities or on their behalf and therefore subject to the requirements and safeguards,\
        \ including human oversight (Article 14 and Article 26 AI Act). For detailed information, please refer to the [guidelines\
        \ on prohibited AI practices, Section 5.2.3, paragraphs 203-206.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_203)\n\
        \n4)\tOut of scope\na)\tLocation-based or geospatial predictive or place-based crime predictions (please refer to\
        \ [Section 5.3.1, paragraphs 212-213](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_212)).\n\
        b)\tAI systems that support human assessments based on objective and verifiable facts linked to a criminal activity\
        \ (please refer to [Section 5.3.2, paragraph 214](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_214)).\n\
        c)\tAI systems used for crime predictions and assessments in relation to legal entities (please refer to [Section\
        \ 5.3.3, paragraphs 215-216](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_215)).\n\
        d)\tAI systems used for individual predictions of administrative offences (please refer to [Section 5.3.4, paragraphs\
        \ 217-218](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_217)).\n"
    - value: '4'
      label: Untargeted AI-enabled scraping of facial images from the internet or CCTV to create or expand databases
      help: "Following the European Commission’s guidelines on prohibited AI practices: \n1)\tFacial recognition databases\n\
        ‘Database’ in this context should be understood to refer to any collection of data, or information, that is specially\
        \ organized for rapid search and retrieval by a computer.\n2)\tThrough untargeted scraping of facial images\n ‘Scraping’\
        \ typically refers to using web crawlers, bots, or other means to extract data or content from different sources,\
        \ including CCTV, websites or social media, automatically. These tools are software ‘ programmed to sift through databases\
        \ and extract information and to make use of that information for another purpose.\n‘Untargeted’ relates to a technique\
        \ that operates like a ‘vacuum cleaner’, absorbing as much data and information as possible, without targeting specifically\
        \ and individually intended subject(s) of the scarping.\nIf a scraping tool is instructed to collect images or video\
        \ containing human faces only of specific individuals or a pre-defined group of persons, then the scraping becomes\
        \ targeted, for example to find one specific criminal or to identify a group of victims.\n3)\tFrom the Internet and\
        \ CCTV footage\nRegarding the internet, the fact that a person has published facial images of themselves on a social\
        \ media platform does not mean that that person has given his or her consent for those images to be included in a\
        \ facial recognition database. Examples of scraping facial images from CCTV footage include images acquired by surveillance\
        \ cameras operated in places such as airports, streets, parks, etc.\n4)\tOut of scope\nThe prohibition in Article\
        \ 5(1)(e) AI Act does not apply to the untargeted scraping of biometric data other than facial images (such as voice\
        \ samples). The prohibition does also not apply where no AI systems are involved in the scraping. Facial image databases\
        \ that are not used for the recognition of persons are also out of scope, such as facial image databases used for\
        \ AI model training or testing purposes, where the persons are not identified.\nThe prohibition in Article 5(1)(e)\
        \ AI Act does not apply to AI systems which harvest large amounts of facial images from the internet to build AI models\
        \ that generate new images about fictitious persons because such systems would not result in the recognition of real\
        \ persons. Such AI systems could fall under the transparency requirements of Article 50 AI Act.\nFor detailed information\
        \ about the conditions relating to Article 5(1)(e), please refer to the [guidelines on prohibited AI practices, Section\
        \ 6, paragraphs 222-238.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_222)\n"
    - value: '5'
      label: AI system deploys emotion recognition in the areas of workplace and education institutions (except when used
        for medical and safety reasons)
      help: "The AI Act defines emotion recognition system as 'an AI system for the purpose of identifying or inferring emotions\
        \ or intentions of natural persons on the basis of their biometric data' (Article 3 (39)).\n'The notion refers to\
        \ emotions or intentions such as happiness, sadness, anger, surprise, disgust, embarrassment, excitement, shame, contempt,\
        \ satisfaction and amusement.' (Recital 18).\nHowever, the notion does not 'include physical states, such as pain\
        \ or fatigue, including, for example, systems used in detecting the state of fatigue of professional pilots or drivers\
        \ for the purpose of preventing accidents. This does also not include the mere detection of readily apparent expressions,\
        \ gestures or movements, unless they are used for identifying or inferring emotions. Those expressions can be basic\
        \ facial expressions, such as a frown or a smile, or gestures such as the movement of hands, arms or head, or characteristics\
        \ of a person's voice, such as a raised voice or whispering.' (Recital 18)\nFollowing the European Commission’s guidelines\
        \ on prohibited AI practices: \n1)\tAI systems to infer emotions\nInferring generally encompasses identifying as a\
        \ prerequisite, so that the prohibition should be understood as including both AI systems identifying or inferring\
        \ emotions or intentions. For consistency reasons, it is also important to construe the prohibition in Article 5(1)(f)\
        \ AI Act as having a similar scope as the rules applicable to other emotion recognition systems (Annex III, point\
        \ 1(c), and Article 50 AI Act) and to limit it to inferences based on a person’s biometric data.\n‘Identification’\
        \ occurs where the processing of the biometric data (for example, of the voice or a facial expression) of a natural\
        \ person allows to directly compare and identify an emotion with one that has been previously programmed in the emotion\
        \ recognition system. ‘Inferring’ is done by deducing information generated by analytical and other processes by the\
        \ system itself.\nFor the purpose of Article 5(1)(f) AI Act, the concept of emotions or intentions should be understood\
        \ in a wide sense and not interpreted restrictively. Recital 18 AI Act provides some detail, listing emotions ‘such\
        \ as happiness, sadness, anger, surprise, disgust, embarrassment, excitement, shame, contempt, satisfaction and amusement’.\
        \ These examples are not exhaustive.\nAccording to the definition in Article 3(39) AI Act, only AI systems identifying\
        \ or inferring emotions or intentions based on biometric data constitute emotion recognition systems. Personal characteristics\
        \ from which biometric data can be extracted are physical or behavioural attributes. Physiological biometrics employ\
        \ physical, structural, and relatively static attributes of a person, such as their fingerprints, the pattern of their\
        \ iris, contours of their face, or the geometry of veins in their hands. Some modalities are microscopic in nature,\
        \ but still exhibit biological and chemical structures that can be acquired and identified e.g., DNA and odour. Behavioural\
        \ biometrics monitor the distinctive characteristics of movements, gestures, and motor-skills of individuals as they\
        \ perform a task or series of tasks. This means that human movements, such as walking (gait analysis) or finger contact\
        \ with a keyboard (keystrokes), are captured and analysed. Behavioural biometrics encompass a variety of modalities\
        \ that exhibit both voluntary and involuntary repeated motions and associated rhythmic timings/pressures of body features\
        \ ranging from signatures, gait, voice, and keystrokes through to eye tracking and heartbeats, electroencephalography\
        \ (EEG), or electrocardiograms (ECG).\nFor detailed information, please refer to the [guidelines on prohibited AI\
        \ practices, Section 7.2.1, paragraphs 244-252.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_244)\n\
        2)\tLimitation of the prohibition to workplace and educational institutions\nThe notion of ’workplace’ should be interpreted\
        \ broadly. That notion relates to any specific physical or virtual space where natural persons engage in tasks and\
        \ responsibilities assigned by their employer or by the organisation they are affiliated to, for example in case of\
        \ self-employment.\nThe reference to education institutions is broad and should be understood to include both public\
        \ and private institutions. There is no limitation as regards the types or ages of pupils or students or of a specific\
        \ environment (online, in person, in a blended mode etc).\nFor detailed information, please refer to the [guidelines\
        \ on prohibited AI practices, Section 7.2.2, paragraphs 253-255.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_253)\n\
        3)\tExceptions for medical and safety reasons\n\nIn particular, therapeutic uses should be understood to mean uses\
        \ of CE-marked medical devices. Moreover, this exception does not comprise the use of emotion recognition systems\
        \ to detect general aspects of wellbeing. The general monitoring of stress levels at the workplace is not permitted\
        \ under health or safety aspects.\nThe notion of safety reasons within this exception should be understood to apply\
        \ only in relation to the protection of life and health and not to protect other interests, for example property against\
        \ theft or fraud.\nIt follows from this narrow interpretation of the exception that any use for medical and safety\
        \ reasons should always remain limited to what is strictly necessary and proportionate, including limits in time,\
        \ personal application and scale, and should be accompanied by sufficient safeguards.\nFor detailed information, please\
        \ refer to the [guidelines on prohibited AI practices, Section 7.2.3, paragraphs 244-252.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_244)\n\
        4)\tOut of scope\nOut of scope are: \n- AI systems inferring emotions and sentiments not on the basis of biometric\
        \ data,\n- AI systems inferring physical states such as pain and fatigue.\nEmotion recognition systems used in all\
        \ other domains other than in the areas of the workplace and education institutions do not fall under the prohibition\
        \ in Article 5(1)(f) AI Act.\nFor detailed information, please refer to the [guidelines on prohibited AI practices,\
        \ Section 7.4, paragraphs 265-270.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_265)\n"
    - value: '6'
      label: Biometric categorisation to deduce sensitive attributes like political opinion, religious beliefs or sexual orientation
      help: "The AI Act refers to biometric categorisation system as 'an AI system for the purpose of assigning natural persons\
        \ to specific categories on the basis of their biometric data, unless it is ancillary to another commercial service\
        \ and strictly necessary for objective technical reasons (Article 3 (40)).\n'Such specific categories can relate to\
        \ aspects such as sex, age, hair colour, eye colour, tattoos, behavioural or personality traits, language, religion,\
        \ membership of a national minority, sexual or political orientation' (Recital 16).\nHowever, the prohibition 'does\
        \ not include biometric categorisation systems that are a purely ancillary feature intrinsically linked to another\
        \ commercial service, meaning that the feature cannot, for objective technical reasons, be used without the principal\
        \ service, and the integration of that feature or functionality is not a means to circumvent the applicability of\
        \ the rules of this Regulation. For example, filters categorising facial or body features used on online marketplaces\
        \ could constitute such an ancillary feature as they can be used only in relation to the principal service which consists\
        \ in selling a product by allowing the consumer to preview the display of the product on him or herself and help the\
        \ consumer to make a purchase decision' (Recital 16).\nFollowing the European Commission’s guidelines on prohibited\
        \ AI practices: \n1)\tBiometric categorisation system\nArticle 3(40) AI Act defines a biometric categorisation system\
        \ as an AI system for the purpose of assigning natural persons to specific categories on the basis of their biometric\
        \ data, unless it is ancillary to another commercial service and strictly necessary for objective technical reasons.\n\
        To fall outside the scope of the definition of biometric categorisation under the AI Act, two conditions – being ‘ancillary\
        \ to another commercial service and strictly necessary for objective technical reasons’ – must be cumulatively fulfilled.\n\
        For detailed information, please refer to the [guidelines on prohibited AI practices, Section 8.2.1, paragraphs 276-280.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_276)\n\
        2)\tPersons are individually categorised based on their biometric data\nFor the prohibition to apply, natural persons\
        \ must be ‘individually’ categorised. If this is not the purpose or outcome of the biometric categorisation, the prohibition\
        \ does not apply, for example if a whole group is categorised without looking at the individual.\nFor detailed information,\
        \ please refer to the [guidelines on prohibited AI practices, Section 8.2.2, paragraphs 281-282.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_284)\n\
        3)\tOut of scope\nThe prohibition in Article 5(1)(g) AI Act does not cover AI systems engaged in the labelling or\
        \ filtering of lawfully acquired biometric datasets. \nThe labelling or filtering of biometric datasets may be done\
        \ by biometric categorisation systems precisely to guarantee that the data equally represent all demographic groups,\
        \ and not, for example, over-represent one specific group. \nArticle 5(1)(f) AI Act also provides that the prohibition\
        \ in that provision does not apply to the labelling or filtering of lawfully acquired datasets in the area of law\
        \ enforcement.\nFor detailed information, please refer to the [guidelines on prohibited AI practices, Section 8.3,\
        \ paragraphs 284-286.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_284)\n"
    - value: '7'
      label: Use of remote biometric identification in 'real-time' in publicly accessible space for the purpose of law enforcement,
        unless explicitly authorised by national law based on the AI Act
      help: "Pursuant to Article 5 (1) (h), the use of 'real-time' remote biometric identification systems in publicly accessible\
        \ spaces for the purposes of law enforcement is prohibited, unless it serves one of the following objectives:\n(i)\
        \ the targeted search for specific victims of abduction, trafficking in human beings or sexual exploitation of human\
        \ beings, as well as the search for missing persons;\n(ii) the prevention of a specific, substantial and imminent\
        \ threat to the life or physical safety of natural persons or a genuine and present or genuine and foreseeable threat\
        \ of a terrorist attack;\n(iii) the localisation or identification of a person suspected of having committed a criminal\
        \ offence, for the purpose of conducting a criminal investigation or prosecution or executing a criminal penalty for\
        \ offences referred to in Annex II and punishable in the Member State concerned by a custodial sentence or a detention\
        \ order for a maximum period of at least four years.\nThe AI Act defines remote biometric identification system as\
        \ 'an AI system for the purpose of identifying natural persons, without their active involvement, typically at a distance\
        \ through the comparison of a person's biometric data with the biometric data contained in a reference database' (Article\
        \ 3 (41)).\nThe AI Act defines real-time remote biometric identification system as 'a remote biometric identification\
        \ system, whereby the capturing of biometric data, the comparison and the identification all occur without a significant\
        \ delay, comprising not only instant identification, but also limited short delays in order to avoid circumvention'\
        \ (Article 3 (42)).\nThe notion of biometric identification biometric identification 'should be defined as the automated\
        \ recognition of physical, physiological and behavioural human features such as the face, eye movement, body shape,\
        \ voice, prosody, gait, posture, heart rate, blood pressure, odour, keystrokes characteristics, for the purpose of\
        \ establishing an individual's identity by comparing biometric data of that individual to stored biometric data of\
        \ individuals in a reference database, irrespective of whether the individual has given its consent or not.' (Recital\
        \ 15)\nFollowing the European Commission’s guidelines on the prohibited AI practices:\n1)\tThe notion of remote biometric\
        \ identification \nThe use of biometric systems to confirm the identity of a natural person for the sole purpose of\
        \ having access to a service, unlocking a device, or having security access to premises is excluded from the concept\
        \ of ‘remote’ (Recital 15 AI Act).\nBiometric recognition systems that process (contactless) fingerprints, gait, voice,\
        \ DNA, keystrokes and other (biometric) behavioural signals may also constitute remote biometric identification systems.\n\
        In the case of body-cams capable of remote biometric identification used by individual law enforcement agents, the\
        \ untargeted filming during, for example, a demonstration with hundreds of participants will be considered to fulfil\
        \ the condition of remoteness.\nFor detailed information, please refer to the [guidelines on prohibited AI practices,\
        \ Section 9.2.1, paragraphs 297-309.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_297)\n\
        2)\tReal-time\nReal-time means that the system captures and further processes biometric data ‘instantaneously, near-instantaneously\
        \ or in any event without any significant delay.’ All the processing steps, i.e. the capture, comparison, and identification\
        \ of biometric data, occur simultaneously or almost simultaneously, which may include a ‘limited short delay’ to avoid\
        \ the prohibition being circumvented through the retrospective use of remote biometric identification systems. The\
        \ notion of ‘without a significant delay’ is not defined in the AI Act; it will have to be assessed on a case-by-case\
        \ basis.\nFor detailed information, please refer to the [guidelines on prohibited AI practices, Section 9.2.2, paragraphs\
        \ 310-312.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_310)\n\
        3)\tIn publicly accessible spaces\nArticle 3(44) AI Act defines publicly accessible spaces as ‘any publicly or privately\
        \ owned physical space accessible to an undetermined number of natural persons, regardless of whether certain conditions\
        \ for access may apply, and regardless of the potential capacity restrictions’. Recital 19 AI Act lists several elements\
        \ that characterise such spaces. \nFor detailed information, please refer to the [guidelines on prohibited AI practices,\
        \ Section 9.2.3, paragraphs 313-318.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_313)\n\
        4)\tFor law enforcement purposes\nThe prohibition in Article 5(1)(h) AI Act applies to the use of remote biometric\
        \ identification systems for law enforcement purposes, irrespective of the entity, authority, or body carrying out\
        \ the law enforcement activities.\nLaw enforcement is defined in Article 3(46) AI Act as the ‘activities carried out\
        \ by law enforcement authorities or on their behalf for the prevention, investigation, detection or prosecution of\
        \ criminal offences or the execution of criminal penalties, including safeguarding against and preventing threats\
        \ to public security.’\nFor detailed information, please refer to the [guidelines on prohibited AI practices, Section\
        \ 9.2.4, paragraphs 319-325.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_319)\n\
        \n5)\tExceptions to the prohibition\na)\tTargeted search for victims of three serious crimes and missing persons (paragraphs\
        \ 329-336)\nb)\tPrevention of imminent threats to life or terrorist attacks (paragraphs 337-348)\nIn application of\
        \ Article 2 of the Charter, which guarantees the right to life, the Union and its Member States must safeguard and,\
        \ thus, protect the lives of individuals. The criteria in Article 5(1)(h)(ii) AI Act concerning the threat to life\
        \ to allow for the use of real-time RBI systems in publicly accessible spaces require the existence of (1) a specific,\
        \ (2) substantial and (3) imminent threat to the life or physical safety of (4) natural persons. The threat does not\
        \ need to be limited to identified individuals or a group, as it relates to natural persons in general. Recital 33\
        \ AI Act clarifies that an imminent threat to life or the physical safety of natural persons may also include an imminent\
        \ threat to critical infrastructure.\nAn imminent threat to life or physical safety is a threat that can occur at\
        \ any moment and requires ‘immediate action to be taken.’ A substantial threat to physical safety relates to serious\
        \ bodily injuries.\nThe assessment concerning the existence and seriousness of the threat is made at national level\
        \ when assessing the actual circumstances of a measure to be taken to safeguard national security, and more specifically,\
        \ in case of a terrorist attack. The terrorist threat level is defined at national level and varies from one Member\
        \ State to another.\nc)\tLocalisation and identification of suspects of certain crimes (paragraphs 349-356)\nArticle\
        \ 5(1)(h)(iii) AI Act covers two categories of individuals: suspects and perpetrators. A suspect is a person with\
        \ regard to whom there are serious grounds for believing that they have committed a criminal offence, and sufficient\
        \ evidence of that person’s involvement in the offence has already been gathered. A perpetrator is a person who is\
        \ accused or convicted of having committed a criminal offence. The same conditions (crime listed in Annex II and maximum\
        \ punishment of at least 4 years) apply to locate or identify the accomplice of the crimes listed in Annex II AI Act.\n\
        6)\tSafeguards and conditions for the exceptions (Article 5(2) – (7) AI Act)\na)\tTarget individual and safeguards\n\
        The use of real-time RBI systems in publicly accessible spaces for law enforcement purposes is only allowed to ‘confirm\
        \ the identity of the specifically targeted individual’.\nThe ‘seriousness’ criterion, applied here in connection\
        \ to the possible harm and consequences, implies a variation in degrees of interference with the fundamental rights\
        \ at stake, which is linked to the principle of proportionality. The ‘scale’ criterion refers, in particular, to the\
        \ number and categories of persons affected by the interference (including children and vulnerable or marginalised\
        \ persons). The ‘probability’ is the likelihood that an event will occur. The assessment of the seriousness, scale\
        \ and probability of the harm and consequences should all be part of the Fundamental Rights Impact Assessment that\
        \ the law enforcement authority is obliged to complete (see below). That assessment will be concluded on a case-by-case\
        \ basis.\nFor detailed information, please refer to the [guidelines on prohibited AI practices, Section 10, paragraphs\
        \ 357-369.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_357)\n\
        For detailed information about:\n- fundamental rights impact assessment, please refer to the [guidelines on prohibited\
        \ AI practices, Section 10.1.1, paragraphs 370-377.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_370)\n\
        - registration of the authorized RBI systems, please refer to the [guidelines on prohibited AI practices, Section\
        \ 10.1.2, paragraphs 378.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_378)\n\
        b) Need for prior authorisation\nArticle 5(3) AI Act requires prior authorisation of each individual use of a real-time\
        \ RBI system and prohibits automated decision-making based solely on the output of such a system which produces an\
        \ adverse legal effect.\nThe use of real-time RBI systems which pursue one of the objectives listed in Article 5(1)(h)(i)\
        \ to (iii) AI Act and which has been provided for in the national law of the Member States concerned must be authorized\
        \ by a judicial authority or an independent administrative authority prior to its use.\nHowever, there is an exception\
        \ in the case of urgency. This shall be duly justified. Urgency is described as ‘situations where the need to use\
        \ the systems concerned is such as to make it effectively and objectively impossible to obtain an authorisation before\
        \ commencing the use of the AI system’ (Recital 35 AI Act).\nFor detailed information, please refer to the [guidelines\
        \ on prohibited AI practices, Section 10.2, paragraphs 379-409.\nc) Notification to the authorities of each use of\
        \ ‘real-time’ remote biometric identification systems in publicly accessible spaces for law enforcement \nEach use\
        \ of an RBI system pursuing one of the objectives listed in Article 5(1)(h)(i)- (iii) AI Act must be notified to the\
        \ relevant market surveillance authority and the national data protection authority. Notification must take place\
        \ after each use in order to be able to report about the number of authorisations and their result.\nd)\tNeed for\
        \ national laws within the limits of the AI Act exceptions\nNational laws are required for operationalising the use\
        \ of ‘real-time’ RBI systems in publicly accessible spaces for the purposes of law enforcement. At the same time,\
        \ Article 5(5) AI Act provides that Member States remain free to decide whether to adopt such national laws. If a\
        \ national law authorising the use of real-time RBI is adopted, the AI Act specifies the substantive elements which\
        \ the national laws must contain to comply with the requirements laid down in the AI Act.\nFor detailed information,\
        \ please refer to the [guidelines on prohibited AI practices, Section 10.4, paragraphs 412-419.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_412)\n\
        e)\tAnnual reports by the national market surveillance authorities and the national data protection authorities of\
        \ Member States (see paragraphs 420-423).\nf)\tAnnual reports by the European Commission (see paragraphs 424-425).\n\
        7)\tOut of scope\nAll other uses of RBI systems that are not covered by the prohibition of Article 5(1)(h) AI Act\
        \ fall within the category of high-risk AI systems as defined by Article 6 and listed in point 1(a) of Annex III AI\
        \ Act provided they fall within the scope of the AI Act.\nFor detailed information, please refer to the [guidelines\
        \ on prohibited AI practices, Section 10.7, paragraphs 426-429.](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-12/guide-prohibited_en.pdf#nameddest=para_426)\n"
    - value: '8'
      label: None of the above
      help: ''
  QAIS 6.1:
    id: QAIS 6.1
    question: Is it regulated under one of the sectors listed below?
    info: ''
    type: multiple_choice
    sources: Article 6 (1); Annex I
    options:
    - value: '0'
      label: Machinery
      help: Regulation (EU) 2023/1230 of the European Parliament and of the Council of 14 June 2023 on machinery and repealing
        Directive 2006/42/EC of the European Parliament and of the Council and Council Directive 73/361/EEC
    - value: '1'
      label: Toys
      help: Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on the safety of toys
    - value: '2'
      label: Recreational craft and personal watercraft
      help: Directive 2013/53/EU of the European Parliament and of the Council of 20 November 2013 on recreational craft and
        personal watercraft
    - value: '3'
      label: Lifts and safety components of lifts
      help: Directive 2014/33/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of
        the laws of the Member States relating to lifts and safety components for lifts
    - value: '4'
      label: Equipment and protective systems intended for use in potentially explosive atmospheres
      help: Directive 2014/34/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of
        the laws of the Member States relating to equipment and protective systems intended for use in potentially explosive
        atmospheres
    - value: '5'
      label: Radio equipment
      help: Directive 2014/53/EU of the European Parliament and of the Council of 16 April 2014 on the harmonisation of the
        laws of the Member States relating to the making available on the market of radio equipment
    - value: '6'
      label: Pressure equipment
      help: Directive 2014/68/EU of the European Parliament and of the Council of 15 May 2014 on the harmonisation of the
        laws of the Member States relating to the making available on the market of pressure equipment
    - value: '7'
      label: Cableway installations
      help: Regulation (EU) 2016/424 of the European Parliament and of the Council of 9 March 2016 on cableway installations
    - value: '8'
      label: Personal protective equipment
      help: Regulation (EU) 2016/425 of the European Parliament and of the Council of 9 March 2016 on personal protective
        equipment
    - value: '9'
      label: Appliances burning gaseous fuels
      help: Regulation (EU) 2016/426 of the European Parliament and of the Council of 9 March 2016 on appliances burning gaseous
        fuels
    - value: '10'
      label: Medical devices
      help: Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices
    - value: '11'
      label: In vitro diagnostic medical devices
      help: Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 on in vitro diagnostic
        medical devices
    - value: '12'
      label: None of the above
      help: ''
  QAIS 6.2:
    id: QAIS 6.2
    question: Is your product, or the product in which your AI system is intended to be used as a safety component is required
      to undergo a third-party conformity assessment under the sectoral law? Note that this also includes products for which
      you can opt-out of a third-party conformity assessment when harmonised standards are fully applied
    info: ''
    type: single_choice
    sources: Article 3, Article 6 (1); Recital 53
    options:
    - value: '0'
      label: 'Yes'
      help: ''
    - value: '1'
      label: 'No'
      help: ''
  QAIS 6.3:
    id: QAIS 6.3
    question: Is your AI system, or the product in which your AI system is intended to be used as a safety component, subject
      to sector-specific regulations and classified under one of the sectors listed below
    info: ''
    type: multiple_choice
    sources: Article 6 (1); Annex I
    options:
    - value: '0'
      label: Civil aviation security
      help: Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on common rules in
        the field of civil aviation security
    - value: '1'
      label: Two- or three-wheel vehicles and quadricycles
      help: Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 January 2013 on the approval and
        market surveillance of two- or three-wheel vehicles and quadricycles
    - value: '2'
      label: Agricultural and forestry vehicles
      help: Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 February 2013 on the approval and
        market surveillance of agricultural and forestry vehicles
    - value: '3'
      label: Marine equipment
      help: Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on marine equipment
    - value: '4'
      label: Interoperability of the rail systems
      help: Directive (EU) 2016/797 of the European Parliament and of the Council of 11 May 2016 on the interoperability of
        the rail system within the European Union/ Regulation (EU) 2019/2144 of the European Parliament and of the Council
        of 27 November 2019 on type-approval requirements for motor vehicles and their trailers, and systems, components and
        separate technical units intended for such vehicles, as regards their general safety and the protection of vehicle
        occupants and vulnerable road users,
    - value: '5'
      label: Motor vehicles and their trailers
      help: Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the approval and market
        surveillance of motor vehicles and their trailers, and of systems, components and separate technical units intended
        for such vehicles
    - value: '6'
      label: Civil aviation
      help: Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in the
        field of civil aviation
    - value: '7'
      label: None of the above
      help: ''
  QAIS 6.4.1:
    id: QAIS 6.4.1
    question: 'Is your AI system intended to be used for any of the following use-cases? Note: the use of your AI system must
      be permitted under relevant EU or national law. '
    info: ''
    type: multiple_choice
    sources: Article 6; Recital 53; Annex III (1)
    options:
    - value: '0'
      label: Remote biometric identification systems
      help: ''
    - value: '1'
      label: AI systems intended to be used for biometric categorisation, according to sensitive or protected attributes or
        characteristics based on the inference of those attributes or characteristics
      help: ''
    - value: '2'
      label: AI systems intended to be used for emotion recognition
      help: ''
    - value: '3'
      label: None of the above
      help: ''
  QAIS 6.4.2:
    id: QAIS 6.4.2
    question: Is your AI system intended to be used for any of the following use-cases?
    info: ''
    type: multiple_choice
    sources: Article 6; Annex III (3)
    options:
    - value: '0'
      label: AI systems intended to be used to determine access or admission or to assign natural persons to educational and
        vocational training institutions at all levels
      help: ''
    - value: '1'
      label: AI systems intended to be used to evaluate learning outcomes, including when those outcomes are used to steer
        the learning process of natural persons in educational and vocational training institutions at all levels
      help: ''
    - value: '2'
      label: AI systems intended to be used for the purpose of assessing the appropriate level of education that an individual
        will receive or will be able to access, in the context of or within educational and vocational training institutions
        at all levels
      help: ''
    - value: '3'
      label: AI systems intended to be used for monitoring and detecting prohibited behaviour of students during tests in
        the context of or within educational and vocational training institutions at all levels
      help: ''
    - value: '4'
      label: None of the above
      help: ''
  QAIS 6.4.3:
    id: QAIS 6.4.3
    question: Is your AI system intended to be used for any of the following use-cases?
    info: ''
    type: single_choice
    sources: Article 6; Annex III (4)
    options:
    - value: '0'
      label: AI systems intended to be used for the recruitment or selection of natural persons, in particular to place targeted
        job advertisements, to analyse and filter job applications, and to evaluate candidates
      help: ''
    - value: '1'
      label: AI systems intended to be used to make decisions affecting terms of work-related relationships, the promotion
        or termination of work-related contractual relationships, to allocate tasks based on individual behaviour or personal
        traits or characteristics or to monitor and evaluate the performance and behaviour of persons in such relationships
      help: ''
    - value: '2'
      label: None of the above
      help: ''
  QAIS 6.4.4:
    id: QAIS 6.4.4
    question: Is your AI system intended to be used for any of the following use-cases?
    info: ''
    type: multiple_choice
    sources: Article 6; Annex III (5)
    options:
    - value: '0'
      label: AI systems intended to be used by public authorities or on behalf of public authorities to evaluate the eligibility
        of natural persons for essential public assistance benefits and services, including healthcare services, as well as
        to grant, reduce, revoke, or reclaim such benefits and services
      help: ''
    - value: '1'
      label: AI systems intended to be used to evaluate the creditworthiness of natural persons or establish their credit
        score, with the exception of AI systems used for the purpose of detecting financial fraud
      help: ''
    - value: '2'
      label: AI systems intended to be used for risk assessment and pricing in relation to natural persons in the case of
        life and health insurance
      help: ''
    - value: '3'
      label: AI systems intended to evaluate and classify emergency calls by natural persons or to be used to dispatch, or
        to establish priority in the dispatching of, emergency first response services, including by police, firefighters
        and medical aid, as well as of emergency healthcare patient triage systems
      help: ''
    - value: '4'
      label: None of the above
      help: ''
  QAIS 6.4.5:
    id: QAIS 6.4.5
    question: Is your AI system intended to be used for any of the following use-cases?
    info: 'Note: the use of your AI system must be permitted under relevant EU or national law.'
    type: multiple_choice
    sources: Article 6; Annex III (6)
    options:
    - value: '0'
      label: AI systems intended to be used by or on behalf of law enforcement authorities, or by Union institutions, bodies,
        offices or agencies in support of law enforcement authorities or on their behalf to assess the risk of a natural person
        becoming the victim of criminal offences
      help: ''
    - value: '1'
      label: AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies,
        offices or agencies in support of law enforcement authorities as polygraphs or similar tools
      help: ''
    - value: '2'
      label: AI systems intended to be used by or on behalf of law enforcement authorities, or by Union institutions, bodies,
        offices or agencies, in support of law enforcement authorities to evaluate the reliability of evidence in the course
        of the investigation or prosecution of criminal offences
      help: ''
    - value: '3'
      label: AI systems intended to be used by law enforcement authorities or on their behalf or by Union institutions, bodies,
        offices or agencies in support of law enforcement authorities for assessing the risk of a natural person offending
        or re-offending not solely on the basis of the profiling of natural persons as referred to in Article 3-(4) of Directive
        (EU) 2016/680, or to assess personality traits and characteristics or past criminal behaviour of natural persons or
        groups
      help: ''
    - value: '4'
      label: AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies,
        offices or agencies in support of law enforcement authorities for the profiling of natural persons as referred to
        in Article 3-(4) of Directive (EU) 2016/680 in the course of the detection, investigation or prosecution of criminal
        offences
      help: ''
    - value: '5'
      label: None of the above
      help: ''
  QAIS 6.4.6:
    id: QAIS 6.4.6
    question: Is your AI system intended to be used for any of the following use-cases?
    info: 'Note: the use of your AI system must be permitted under relevant EU or national law.'
    type: multiple_choice
    sources: Article 6; Annex III (7)
    options:
    - value: '0'
      label: AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies,
        offices or agencies as polygraphs or similar tools
      help: ''
    - value: '1'
      label: AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies,
        offices or agencies to assess a risk, including a security risk, a risk of irregular migration, or a health risk,
        posed by a natural person who intends to enter or who has entered into the territory of a Member State
      help: ''
    - value: '2'
      label: AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies,
        offices or agencies to assist competent public authorities for the examination of applications for asylum, visa or
        residence permits and for associated complaints with regard to the eligibility of the natural persons applying for
        a status, including related assessments of the reliability of evidence
      help: ''
    - value: '3'
      label: AI systems intended to be used by or on behalf of competent public authorities, or by Union institutions, bodies,
        offices or agencies, in the context of migration, asylum or border control management, for the purpose of detecting,
        recognising or identifying natural persons, with the exception of the verification of travel documents
      help: ''
    - value: '4'
      label: None of the above
      help: ''
  QAIS 6.4.7:
    id: QAIS 6.4.7
    question: Is your AI system intended to be used for any of the following use-cases?
    info: ''
    type: single_choice
    sources: Article 6; Annex III (8)
    options:
    - value: '0'
      label: AI systems intended to be used by a judicial authority or on their behalf to assist a judicial authority in researching
        and interpreting facts and the law and in applying the law to a concrete set of facts, or to be used in a similar
        way in alternative dispute resolution
      help: ''
    - value: '1'
      label: AI systems intended to be used for influencing the outcome of an election or referendum or the voting behaviour
        of natural persons in the exercise of their vote in elections or referenda. This does not include AI systems to the
        output of which natural persons are not directly exposed, such as tools used to organise, optimise or structure political
        campaigns from an administrative or logistical point of view
      help: ''
    - value: '2'
      label: None of the above
      help: ''
  QAIS 6.4:
    id: QAIS 6.4
    question: Does your AI system fall within any of the following areas?
    info: ''
    type: multiple_choice
    sources: Article 6 (2); Annex III; Recital 52; Recital 48; Recital 54; Recital 55; Recital 56; Recital 57; Recital 58;
      Recital 59; Recital 60; Recital 61; Recital 62; Recital 63
    options:
    - value: '0'
      label: 'Critical infrastructure: AI systems intended to be used as safety components in the management and operation
        of critical digital infrastructure, road traffic, or in the supply of water, gas, heating or electricity'
      help: See Annex III (2)
    - value: '1'
      label: Biometrics
      help: ''
    - value: '2'
      label: Education and vocational training
      help: ''
    - value: '3'
      label: Employment, workers' management and access to self-employment
      help: ''
    - value: '4'
      label: Access to and enjoyment of essential private services and essential public services and benefits
      help: ''
    - value: '5'
      label: Law enforcement
      help: ''
    - value: '6'
      label: Migration, asylum and border control management
      help: ''
    - value: '7'
      label: Administration of justice and democratic processes
      help: ''
    - value: '8'
      label: None of the above
      help: ''
  QAIS 6.5:
    id: QAIS 6.5
    question: 'Could your AI system be exempted from classifying as high-risk because it does not pose a significant risk
      of harm to the health, safety or fundamental rights of natural persons? Note: an AI system referred to in Annex III
      shall always be considered to be high-risk where the AI system performs profiling of natural persons (Article 6 (3)
      AI Act)'
    info: 'An AI system referred to in Annex III shall not be considered to be high-risk where it does not pose a significant
      risk of harm to the health, safety or fundamental rights of natural persons, including by not materially influencing
      the outcome of decision making. The first subparagraph shall apply where any of the following conditions is fulfilled:
      (a) the AI system is intended to perform a narrow procedural task; (b) the AI system is intended to improve the result
      of a previously completed human activity; (c) the AI system is intended to detect decision-making patterns or deviations
      from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment,
      without proper human review; or (d) the AI system is intended to perform a preparatory task to an assessment relevant
      for the purposes of the use cases listed in Annex III. Notwithstanding the first subparagraph, an AI system referred
      to in Annex III shall always be considered to be high-risk where the AI system performs profiling of natural persons
      (Article 6 (3)).'
    type: multiple_choice
    sources: Article 6 (3)
    options:
    - value: '0'
      label: Yes. The AI system is intended to perform a narrow procedural task
      help: ''
    - value: '1'
      label: Yes. The AI system is intended to improve the result of a previously completed human activity
      help: ''
    - value: '2'
      label: Yes. The AI system is intended to detect decision-making patterns or deviations from prior decision-making patterns
        and is not meant to replace or influence the previously completed human assessment, without proper human review
      help: ''
    - value: '3'
      label: Yes. The AI system is intended to perform a preparatory task to an assessment relevant for the purposes of the
        use cases listed in Annex III
      help: ''
    - value: '4'
      label: None of the above / The system performs profiling of natural person
      help: ''
  QAIS 6:
    id: QAIS 6
    question: Is your AI system, or the product in which it is intended to be used as a safety component, subject to sector-specific
      regulations?
    info: The AI Act defines safety component as 'a component of a product or of an AI system which fulfils a safety function
      for that product or AI system, or the failure or malfunctioning of which endangers the health and safety of persons
      or property' (Article 3 (14)).
    type: single_choice
    sources: Article 3 (14); Article 6
    options:
    - value: '0'
      label: 'Yes'
      help: ''
    - value: '1'
      label: 'No'
      help: ''
    - value: '2'
      label: Uncertain
      help: ''
  QAIS 7.1:
    id: QAIS 7.1
    question: Does your AI system perform any of these functions?
    info: ''
    type: single_choice
    sources: Article 50; Recital 132; Recital 133
    options:
    - value: '0'
      label: The AI system interacts directly with natural persons
      help: Providers shall ensure that AI systems intended to interact directly with natural persons are designed and developed
        in such a way that the natural persons concerned are informed that they are interacting with an AI system, unless
        this is obvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect,
        taking into account the circumstances and the context of use. This obligation shall not apply to AI systems authorised
        by law to detect, prevent, investigate or prosecute criminal offences, subject to appropriate safeguards for the rights
        and freedoms of third parties, unless those systems are available for the public to report a criminal offence (Article
        50 (1)).
    - value: '1'
      label: The AI system generates synthetic audio, image, video or text content
      help: '''This (…) shall not apply to the extent the AI systems perform an assistive function for standard editing or
        do not substantially alter the input data provided by the deployer or the semantics thereof, or where authorised by
        law to detect, prevent, investigate or prosecute criminal offences'' (Article 50 (2))'
    - value: '2'
      label: None of the above
      help: ''
  QAIS 7.2:
    id: QAIS 7.2
    question: Does your AI system perform any of these functions?
    info: ''
    type: multiple_choice
    sources: Article 50; Recital 132; Recital 134
    options:
    - value: '0'
      label: The AI system is an emotion recognition system or a biometric categorisation system
      help: '''This (…) shall not apply to AI systems used for biometric categorisation and emotion recognition, which are
        permitted by law to detect, prevent or investigate criminal offences, subject to appropriate safeguards for the rights
        and freedoms of third parties, and in accordance with Union law'' (Article 50 (3)).'
    - value: '1'
      label: The AI system generates or manipulates image, audio or video content that would falsely appear to a person to
        be authentic (deep fake)
      help: ''
    - value: '2'
      label: The AI system generates or manipulates text that is published with the purpose of informing the public on matters
        of public interest
      help: ''
    - value: '3'
      label: None of the above
      help: ''
  QAIS 8:
    id: QAIS 8
    question: Is your AI system released under free and open-source licences?
    info: Free and open-source AI components covers the software and data, including models and general-purpose AI models,
      tools, services or processes of an AI system. Free and open-source AI components can be provided through different channels,
      including their development on open repositories. For the purposes of the AI Act, AI components that are provided against
      a price or otherwise monetised, including through the provision of technical support or other services, including through
      a software platform, related to the AI component, or the use of personal data for reasons other than exclusively for
      improving the security, compatibility or interoperability of the software, with the exception of transactions between
      microenterprises, should not benefit from the exceptions provided to free and open-source AI components. The fact of
      making AI components available through open repositories should not, in itself, constitute a monetisation (Recital 103).
    type: single_choice
    sources: Article 2 (12); Recital 103
    options:
    - value: '0'
      label: 'Yes'
      help: ''
    - value: '1'
      label: 'No'
      help: ''
  QGPAI 1:
    id: QGPAI 1
    question: 'Does your AI model qualify as a general-purpose AI model under the AI Act? '
    info: 'The AI Act defines a general-purpose AI model as an ''AI model, including where such an AI model is trained with
      a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently
      performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated
      into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping
      activities before they are placed on the market'' (Article 3 (63)).


      The European Commission''s [guidelines on the scope of the obligations for general-purpose AI models](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-07/guidelines_on_the_scope_of_the_obligations_for_generalpurpose_ai_models_established_by_regulation_1cx2atxgq79us4n3x8jfgyy1qlm_118340-3.pdf#page=6)
      in Section 2.1 provide an indicative criterion to help determine whether the Commission considers an AI model to be
      a general-purpose AI (‘GPAI’) model: if its training compute exceeds 10^23 FLOP and if it can generate language (text
      or audio), text-to-image, or text-to-video. However, this is not an absolute rule - models meeting this criterion may
      exceptionally not be considered GPAI if they lack significant generality, while models not meeting this criterion may
      still be considered GPAI if they display significant generality and can competently perform a wide range of tasks.'
    type: single_choice
    sources: ''
    options:
    - value: '0'
      label: 'Yes'
      help: ''
    - value: '1'
      label: 'No'
      help: ''
  QGPAI 2:
    id: QGPAI 2
    question: Which of the following statements apply to your general-purpose AI model?
    info: 'A general-purpose AI model is classified as presenting systemic risk through two pathways:


      First, if it has capabilities that match or exceed those of the most advanced models (‘high-impact capabilities’). The
      AI Act presumes that general-purpose AI models trained with a cumulative amount of compute greater than 10^25 FLOP have
      such capabilities (Article 51(2) AI Act). The European Commission can adjust this threshold to account for technological
      advancements (Article 51(3) AI Act).


      Second, the European Commission can designate a general-purpose AI model as presenting systemic risk, either on its
      own initiative or following a qualified alert from the scientific panel, if the general-purpose AI model has capabilities
      or impact equivalent to those of the most advanced models, having regard to the criteria established in Annex XIII,
      AI Act. This allows for case-by-case assessment of models that may present systemic risks despite not meeting the compute
      threshold. The European Commission can also designate a general-purpose AI model with systemic risk that meets the compute
      threshold if it becomes aware of such model and the provider has not notified the European Commission (Article 52(1)
      AI Act).


      When a general-purpose AI model has met, or it becomes known that it will meet a requirement leading to the presumption
      that the model has high-impact capabilities, the provider must notify the European Commission in line with Article 52(1)
      AI Act (see also Recital 112 AI Act). All providers of general-purpose AI model with systemic risk, independently from
      how the classification occurred, must comply with additional obligations including assessing and mitigating systemic
      risks, performing model evaluations, reporting serious incidents, and ensuring an adequate level of cybersecurity for
      the model and its physical infrastructure (Article 55(1) AI Act).


      For more information, we refer to Section 2.3 ''When is a general-purpose AI model a general-purpose AI model with systemic
      risk?'' of the European Commission''s [guidelines on the scope of the obligations for general-purpose AI models](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-07/guidelines_on_the_scope_of_the_obligations_for_generalpurpose_ai_models_established_by_regulation_1cx2atxgq79us4n3x8jfgyy1qlm_118340-3.pdf#page=11).'
    type: single_choice
    sources: ''
    options:
    - value: '0'
      label: 'The model has high-impact capabilities '
      help: ''
    - value: '1'
      label: 'The Commission designated the model as presenting systemic risk '
      help: ''
    - value: '2'
      label: None of the above
      help: ''
  QGPAI 3:
    id: QGPAI 3
    question: 'What is your role? '
    info: The AI Act establishes different roles in the general-purpose AI model value chain. A 'provider' is a natural or
      legal person, public authority, agency, or other body that develops a general-purpose AI model or has one developed
      and places it on the market under its own name or trademark (Article 3(3) AI Act). An 'authorised representative' is
      a natural or legal person located or established in the Union who has received a written mandate from a provider of
      a general-purpose AI model to perform the obligations under the AI Act on their behalf (Article 3(5) AI Act). A 'downstream
      modifier' is an actor (distinct from the original provider) who modifies a general-purpose AI model that has already
      been placed on the market, that may, in certain cases, be considered a provider.
    type: multiple_choice
    sources: ''
    options:
    - value: '0'
      label: Provider of a general-purpose AI model
      help: 'You develop a general-purpose AI model (or have it developed) and place it on the Union market under your own
        name or trademark, whether for payment or free of charge. ''Placing on the market'' means the first making available
        on the Union market, whether through APIs, downloads, cloud services, integration into applications, or other means.
        This applies even when models are integrated into AI systems, or used for internal processes that are essential for
        providing a product or service to third parties or that affect the rights of natural persons in the Union (Recital
        97 and Article 3(9) and (10) AI Act).


        The European Commission’s [guidelines on the scope of the obligations for general-purpose AI models](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-07/guidelines_on_the_scope_of_the_obligations_for_generalpurpose_ai_models_established_by_regulation_1cx2atxgq79us4n3x8jfgyy1qlm_118340-3.pdf#page=18)
        provide extensive examples of what actions constitute placing on the market in Section 3.1.2 ''Examples of placing
        on the market of general-purpose AI models''.'
    - value: '1'
      label: 'Authorised representative of a provider of a general-purpose AI model '
      help: You are located or established in the Union and have received a written mandate from a provider of a general-purpose
        AI model (established outside the Union) to act on their behalf for compliance with the AI Act (Article 3(5) AI Act).
    - value: '2'
      label: Downstream modifier of a general-purpose AI model
      help: 'You modify a general-purpose AI model from another provider and place it on the Union market under your own name
        or trade mark, whether for payment or free of charge. You are not the original provider and not acting on their behalf. '
    - value: '3'
      label: None of the above
      help: ''
  QGPAI 3.1:
    id: QGPAI 3.1
    question: Is your modification leading to a significant change in the model’s generality, capabilities, or systemic risk?
    info: When a downstream actor modifies a general-purpose AI model, they may become the provider of the modified model
      if the modification leads to a significant change in the model’s generality, capabilities, or systemic risk. According
      to the European Commission's [guidelines on the scope of the obligations for general-purpose AI models](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-07/guidelines_on_the_scope_of_the_obligations_for_generalpurpose_ai_models_established_by_regulation_1cx2atxgq79us4n3x8jfgyy1qlm_118340-3.pdf#page=19),
      this is expected to occur when the training compute used for the modification exceeds a third of the training compute
      of the original model. If you cannot determine the original model's training compute, use a third of 10^25 FLOP for
      models with systemic risk, or a third of 10^23 FLOP for other general-purpose AI models. However, this is not an absolute
      rule – downstream actors not meeting this criterion may still be considered providers if the modification leads to a
      significant change in the model’s generality, capabilities, or systemic risk.
    type: single_choice
    sources: ''
    options:
    - value: '0'
      label: 'Yes'
      help: ''
    - value: '1'
      label: 'No'
      help: ''
  QGPAI 7:
    id: QGPAI 7
    question: 'Does your model qualify for open-source exceptions? '
    info: 'A provider of a general-purpose AI model may not have to comply with some obligations if this model is released
      under a free and open-source license allowing access, use, modification, and distribution without monetisation (Article
      53(2) and Article 54(6) AI Act). Moreover, its parameters, including weights, architecture, and usage information must
      be publicly available.


      For more information, we refer to Section 4 ''Exceptions from certain obligations for certain models released as open-source''
      of the European Commission’s [guidelines on the scope of the obligations for general-purpose AI models](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-07/guidelines_on_the_scope_of_the_obligations_for_generalpurpose_ai_models_established_by_regulation_1cx2atxgq79us4n3x8jfgyy1qlm_118340-3.pdf#page=22).'
    type: single_choice
    sources: ''
    options:
    - value: '0'
      label: 'Yes'
      help: ''
    - value: '1'
      label: 'No'
      help: ''
results:
  flag_outofscope:
    id: flag_outofscope
    description: Your AI system is likely to fall outside the scope of the AI Act, according to Article 2. However, please
      double check with the AI Act provisions or consult a lawyer to verify this.
  flag_notaimodel_result_output:
    id: flag_notaimodel_result_output
    description: Your AI model likely does not qualify as a general-purpose AI model. The AI Act does not introduce any obligation
      for providers of AI models that do not qualify as general-purpose AI models.
  flag_aimodel_obligations_result_output:
    id: flag_aimodel_obligations_result_output
    description: ''
  flag_risklevel_output_gpai_with_systemic_risk:
    id: flag_risklevel_output_gpai_with_systemic_risk
    description: Your AI model likely qualifies as a general-purpose AI model with systemic risk.
  flag_risklevel_output_gpai_without_systemic_risk:
    id: flag_risklevel_output_gpai_without_systemic_risk
    description: Your AI model likely qualifies as a general-purpose AI model, but it’s unlikely to be classified as a general-purpose
      AI model with systemic risk.
  flag_aimodel_obligations_systemicrisk_result_output:
    id: flag_aimodel_obligations_systemicrisk_result_output
    description: 'Your AI model likely qualifies as a general-purpose AI model. In that case, you must comply with the obligations
      under Articles 53 and 54 AI Act, which means:


      - draw up and maintain technical documentation about the model, including details of the development process, to provide
      to the AI Office upon request. National competent authorities can also ask the AI Office to request information on their
      behalf when this information is needed for their supervisory tasks (see further Article 53(1)(a), Annex XI AI Act);

      - provide information and documentation to downstream AI system providers to help them understand the model''s capabilities
      and limitations and comply with their own obligations (see further Article 53(1)(b), Annex XII AI Act);

      - put in place a policy to comply with Union law on copyright and related rights, and in particular identify and comply
      with, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of
      Directive (EU) 2019/790 (see further Article 53(1)(c) AI Act);

      - draw up and publish a sufficiently detailed summary of the content used for training the model (see further Article
      53(1)(d) AI Act), according to the [template for general-purpose AI model providers to summarise their training content](https://digital-strategy.ec.europa.eu/en/news/commission-presents-template-general-purpose-ai-model-providers-summarise-data-used-train-their);

      - cooperate as necessary with the Commission and the national competent authorities in the exercise of their competences
      and powers (see further Article 53(3) AI Act);

      - if established outside the EU, appoint an authorised representative in the Union before placing their model on the
      market (see further Article 54 AI Act).


      Your general-purpose AI model is also likely to present systemic risk. In that case, you must comply with the additional
      obligations under Article 55 AI Act, namely:


      - perform model evaluations using standardised protocols and state-of-the-art tools, including conducting and documenting
      adversarial testing of the model to identify and mitigate systemic risks (see further Article 55(1)(a) AI Act);

      - assess and mitigate possible systemic risks at Union level, including their sources, which may stem from the development,
      placing on market, or use of the model (see further Article 55(1)(b) AI Act);

      - track, document, and report, without undue delay, relevant information about serious incidents and possible corrective
      measures to the AI Office and, as appropriate, national competent authorities (see further Article 55(1)(c) AI Act);

      - ensure adequate cybersecurity protection for both the model and its physical infrastructure (see further Article 55(1)(d)
      AI Act).


      You may sign and adhere to the [General-Purpose AI Code of Practice](https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai)
      to demonstrate compliance with the obligations listed above pursuant to Articles 53(1)(a), (b), and (c), and 55(1) AI
      Act.'
  flag_aimodel_obligations_nosystemicrisk_result_output:
    id: flag_aimodel_obligations_nosystemicrisk_result_output
    description: 'Your AI model likely qualifies as a general-purpose AI model. In that case, you must comply with the obligations
      under Articles 53 and 54 AI Act, which means:


      - draw up and maintain technical documentation about the model, including details of the development process, to provide
      to the AI Office upon request. National competent authorities can also ask the AI Office to request information on their
      behalf when this information is needed for their supervisory tasks (see further Article 53(1)(a), Annex XI AI Act);

      - provide information and documentation to downstream AI system providers to help them understand the model''s capabilities
      and limitations and comply with their own obligations (see further Article 53(1)(b), Annex XII AI Act);

      - put in place a policy to comply with Union law on copyright and related rights, and in particular identify and comply
      with, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of
      Directive (EU) 2019/790 (see further Article 53(1)(c) AI Act);

      - draw up and publish a sufficiently detailed summary of the content used for training the model (see further Article
      53(1)(d) AI Act), according to the [template for general-purpose AI model providers to summarise their training content](https://digital-strategy.ec.europa.eu/en/news/commission-presents-template-general-purpose-ai-model-providers-summarise-data-used-train-their);

      - cooperate as necessary with the Commission and the national competent authorities in the exercise of their competences
      and powers (see further Article 53(3) AI Act);

      - if established outside the EU, appoint an authorised representative in the Union before placing their model on the
      market (see further Article 54 AI Act).


      You may sign and adhere to the Transparency and Copyright Chapters of the [General-Purpose AI Code of Practice](https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai)
      to demonstrate compliance with the obligations listed above pursuant to Article 53(1)(a), (b), and (c) AI Act.'
  flag_obligations_provideranddeployer_ailiteracy:
    id: flag_obligations_provideranddeployer_ailiteracy
    description: According to Article 4, you must ensure a sufficient level of AI literacy of your staff and other persons
      dealing with the operation and use of AI systems on your behalf, taking into account their technical knowledge, experience,
      education and training and the context the AI systems are to be used in, and considering the persons or groups of persons
      on whom the AI systems are to be used.
  flag_role_provider_gpai_output:
    id: flag_role_provider_gpai_output
    description: According to the AI Act, you are considered a provider of a general-purpose AI model under Article 3(3).
  flag_role_authorized_representative_gpai_output:
    id: flag_role_authorized_representative_gpai_output
    description: According to the AI Act, you are considered an authorised representative under Article 3(5).
  flag_obligations_gpai_outofscope_solescientific:
    id: flag_obligations_gpai_outofscope_solescientific
    description: ''
  flag_obligations_gpai_outofscope_researchtesting:
    id: flag_obligations_gpai_outofscope_researchtesting
    description: ''
  flag_obligations_opensource_provider:
    id: flag_obligations_opensource_provider
    description: 'Your general-purpose AI model likely qualifies for the open-source exception. Therefore, you must only comply
      with the following obligations under Article 53 AI Act:


      - put in place a policy to comply with Union law on copyright and related rights, and in particular identify and comply
      with, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of
      Directive (EU) 2019/790 (see further Article 53(1)(c) AI Act). You may sign and adhere to the Copyright Chapter of the
      [General-Purpose AI Code of Practice](https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai) to demonstrate
      compliance;

      - draw up and publish a sufficiently detailed summary of the content used for training the model (see further Article
      53(1)(d) AI Act), according to the [template for general-purpose AI model providers to summarise their training content](https://digital-strategy.ec.europa.eu/en/news/commission-presents-template-general-purpose-ai-model-providers-summarise-data-used-train-their);

      - cooperate as necessary with the European Commission and the national competent authorities in the exercise of their
      competences and powers (see further Article 53(3) AI Act).'
  flag_risklevel_aisystem_output:
    id: flag_risklevel_aisystem_output
    description: Your AI system is likely covered by the AI Act as it does fall within the scope of Article 3 (1).
  flag_risklevel_aisystem_nohighrisk_output:
    id: flag_risklevel_aisystem_nohighrisk_output
    description: Your AI system is likely not considered as high-risk. It is likely that your AI system poses no to minimal
      risks.
  flag_ai_system_outsidescope:
    id: flag_ai_system_outsidescope
    description: Your AI system is likely not covered by the AI Act as it does not fall within the scope of Article 3 (1).
  flag_ai_system_role_provider:
    id: flag_ai_system_role_provider
    description: According to the AI Act, you are considered as a provider under Article 3 (3).
  flag_ai_system_role_deployer:
    id: flag_ai_system_role_deployer
    description: According to the AI Act, you are considered as a deployer under Article 3 (4).
  flag_ai_system_role_importer:
    id: flag_ai_system_role_importer
    description: According to the AI Act, you are considered as an importer under Article 3 (6).
  flag_ai_system_role_distributor:
    id: flag_ai_system_role_distributor
    description: According to the AI Act, you are considered as a distributor under Article 3 (7).
  flag_ai_system_role_authorisedrepre:
    id: flag_ai_system_role_authorisedrepre
    description: According to the AI Act, you are considered as an authorised representative under Article 3 (5).
  flag_ai_system_role_productmanufacturer:
    id: flag_ai_system_role_productmanufacturer
    description: According to the AI Act, you are considered as a product manufacturer under Articles 2 (1) (e) and 3 (8).
  flag_aisystem_role_and_obligation_outofscope:
    id: flag_aisystem_role_and_obligation_outofscope
    description: The AI Act does not apply to natural persons using AI systems in the course of a purely personal non-professional
      activity (Article 2 (10)). Please note that any activity through which a natural person gains an economic benefit on
      a regular basis or is otherwise involved in a professional, business, trade, occupational or freelance activity should
      be considered as a ‘professional’ activity.
  flag_role_distriimportdeployer_becomeprovider_result:
    id: flag_role_distriimportdeployer_becomeprovider_result
    description: Under Article 25 (1) of the AI Act, even though you initially identified yourself as a distributor, importer
      or deployer, you are likely considered a provider and must therefore meet the obligations associated with a provider.
  flag_role_manutoprovider_output:
    id: flag_role_manutoprovider_output
    description: Under Article 25 (3) of the AI Act, even though you initially identified yourself as a product manufacturer,
      you are likely considered a provider and must therefore meet the obligations associated with a provider.
  flag_aisystem_obligations_exclusions_output:
    id: flag_aisystem_obligations_exclusions_output
    description: It is likely that your AI system falls outside the scope of the AI Act. You are not required to comply with
      the AI Act because your system is classified as one of the excluded systems mentioned in Article 2.
  flag_obligations_prohibitedsystems_result_output:
    id: flag_obligations_prohibitedsystems_result_output
    description: Your AI system is likely prohibited under the AI Act, according to Article 5. As of February 2025, you are
      not allowed to place your AI system on the market, put it into service, or use it.
  flag_risklevel_aisystem_highrisk_output:
    id: flag_risklevel_aisystem_highrisk_output
    description: Your AI system is likely a high-risk system under the AI Act following Article 6.
  flag_obligations_provider_results_high_risk:
    id: flag_obligations_provider_results_high_risk
    description: "Your AI system is likely classified as a high-risk system under the AI Act, following Article 6. You are\
      \ required to comply with the obligations outlined in Articles 8-15 of the AI Act, which pertain to high-risk AI systems.\
      \ These requirements include:\n-\testablish a risk management system;\n-\tensure adequate data governance and management\
      \ practices;\n-\tdraw up technical documentation;\n-\tensure traceability (record-keeping);\n-\tenable deployers to\
      \ interpret your AI system's output;\n-\tenable human oversight;\n-\tensure an appropriate level of accuracy, robustness\
      \ and cybersecurity.\nAdditionally, as indicated in Article 16, a provider must also:\n-\tindicate on the high-risk\
      \ AI system or, where that is not possible, on its packaging or its accompanying documentation, as applicable, their\
      \ name, registered trade name or registered trademark, the address at which they can be contacted;\n-\thave a quality\
      \ management system in place which complies with Article 17;\n-\tkeep the documentation referred to in Article 18; \n\
      -\twhen under their control, keep the logs automatically generated by their high-risk AI systems as referred to in Article\
      \ 19;\n-\tensure that the high-risk AI system undergoes the relevant conformity assessment procedure as referred to\
      \ in Article 43;\n-\tdraw up an EU declaration of conformity in accordance with Article 47;\n-\taffix the CE marking\
      \ to the high-risk AI system, in accordance with Article 48;\n-\tcomply with the registration obligations referred to\
      \ in Article 49(1);\n-\ttake the necessary corrective actions and provide information as required in Article 20;\n-\t\
      cooperate with national competent authorities as required in Article 21.\n-\tensure that the high-risk AI system complies\
      \ with accessibility requirements in accordance with Directives (EU) 2016/2102 and (EU) 2019/882.  \nIf you have selected\
      \ several sectors within Annex III and one of the use cases is considered high-risk and the other(s) not, compliance\
      \ with the whole Chapter 3 applies only to the high-risk use case you have selected and the requirements must be assessed\
      \ in view of that intended purpose"
  flag_obligations_authorisdrepre_results_high_risk:
    id: flag_obligations_authorisdrepre_results_high_risk
    description: 'Your AI system is likely classified as a high-risk system under the AI Act, following Article 6. As an authorised
      representative of a provider of a high-risk AI system, you must, pursuant to Article 22:


      - verify that the EU declaration of conformity referred to in Article 47 and the technical documentation referred to
      in Article 11 have been drawn up and that an appropriate conformity assessment procedure has been carried out by the
      provider;

      - keep at the disposal of the competent authorities, for a period of 10 years after the high-risk AI system has been
      placed on the market or put into service, the contact details of the provider that appointed the authorised representative,
      a copy of the EU declaration of conformity referred to in Article 47, the technical documentation and, if applicable,
      the certificate issued by the notified body;

      - provide a competent authority, upon a reasoned request, with all the information and documentation necessary to demonstrate
      the conformity of a high-risk AI system with the requirements set out in the AI Act;

      - cooperate with competent authorities;

      - terminate the mandate if you consider or have reason to consider the provider to be acting contrary to its obligations
      pursuant to the AI Act.'
  flag_obligations_importer_output:
    id: flag_obligations_importer_output
    description: "Your AI system is likely classified as a high-risk system under the AI Act, following Article 6. As an importer\
      \ of a high-risk AI system, before placing a high-risk AI system on the market, pursuant to Article 23 you must:\n-\t\
      verify the relevant conformity assessment procedure referred to in Article 43 has been carried out by the provider of\
      \ the high-risk AI system;\n-\tverify that the provider has drawn up the technical documentation in accordance with\
      \ Article 11 and Annex IV;\n-\tverify that the system bears the required CE marking and is accompanied by the EU declaration\
      \ of conformity referred to in Article 47 and instructions for use;\n-\tverify the provider has appointed an authorised\
      \ representative in accordance with Article 22(1);\nAdditionally, as indicated in Article 23, an importer must also;\n\
      -\tnot place the AI system on the market if there is sufficient reason to consider that a high-risk AI system is not\
      \ in conformity with the AI Act, or is falsified, or accompanied by falsified documentation;\n-\tindicate your name,\
      \ registered trade name or registered trade mark, and the address at which you can be contacted on the high-risk AI\
      \ system and on its packaging or its accompanying documentation;\n-\tensure that storage or transport conditions, where\
      \ applicable, do not jeopardise compliance with the AI Act;\n-\tkeep for a period of 10 years after the high-risk AI\
      \ system has been placed on the market or put into service, a copy of the certificate issued by the notified body, where\
      \ applicable, of the instructions for use, and of the EU declaration of conformity referred to in Article 47;\n-\tcooperate\
      \ with competent authorities."
  flag_obligations_distributor_output:
    id: flag_obligations_distributor_output
    description: "Your AI system is likely classified as a high-risk system under the AI Act, following Article 6. As a distributor\
      \ of a high-risk AI system, before placing a high-risk AI system on the market, pursuant to Article 24 you must:\n-\t\
      verify that the high-risk AI system bears the required CE marking, that it is accompanied by a copy of the EU declaration\
      \ of conformity referred to in Article 47 and instructions for use, and that the provider and the importer of that system\
      \ have complied with their respective obligations;\n-\tdo not make the high-risk AI system available on the market if\
      \ there is reason to consider that the AI system is not in conformity with the requirements set out in the AI Act;\n\
      -\tensure that storage or transport conditions, where applicable, do not jeopardise compliance with the AI Act;\n-\t\
      take corrective actions necessary to bring into conformirty a high-risk system that is not compliant with the AI Act;\n\
      -\tcooperate with competent authorities."
  flag_obligations_deployer_output:
    id: flag_obligations_deployer_output
    description: "Your AI system is likely classified as a high-risk system under the AI Act, following Article 6. As a deployer\
      \ of a high-risk AI system, pursuant to Article 26 you must:\n-\ttake appropriate technical and organisational measures\
      \ to ensure that AI systems are used in accordance with the instructions accompanying the AI systems; \n-\tassign human\
      \ oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support;\n\
      -\tto the extent the deployer exercises control over the input data, you shall ensure that input data is relevant and\
      \ sufficiently representative in view of the intended purpose of the high-risk AI system;\n-\tmonitor the operation\
      \ of the high-risk AI system on the basis of the instructions for use and, where relevant, inform providers in accordance\
      \ with Article 72. Where there is a reason to consider that the use of the high-risk AI system in accordance with the\
      \ instructions may result in that AI system presenting a risk you must without undue delay inform the provider or distributor\
      \ and the relevant market surveillance authority, and shall suspend the use of that system; \n-\tkeep the logs automatically\
      \ generated by that high-risk AI system to the extent such logs are under their control, for a period appropriate to\
      \ the intended purpose of the high-risk AI system, of at least six months, unless provided otherwise in applicable Union\
      \ or national law, in particular in Union law on the protection of personal data. Deployers that are financial institutions\
      \ subject to requirements regarding their internal governance, arrangements or processes under Union financial services\
      \ law shall maintain the logs as part of the documentation kept pursuant to the relevant Union financial service law.\n\
      Additionally:\n-\tfor high-risk AI system at the workplace, deployers who are employers shall inform workers' representatives\
      \ and the affected workers that they will be subject to the use of the high-risk AI system;\n-\tin the framework of\
      \ an investigation for the targeted search of a person suspected or convicted of having committed a criminal offence,\
      \ the deployer of a high-risk AI system for post-remote biometric identification shall request an authorisation, ex-ante,\
      \ or without undue delay and no later than 48 hours, by a judicial authority or an administrative authority whose decision\
      \ is binding and subject to judicial review, for the use of that system, except when it is used for the initial identification\
      \ of a potential suspect based on objective and verifiable facts directly linked to the offence. Each use shall be limited\
      \ to what is strictly necessary for the investigation of a specific criminal offence;\n-\tdeployers of high-risk AI\
      \ systems referred to in Annex III that make decisions or assist in making decisions related to natural persons shall\
      \ inform the natural persons that they are subject to the use of the high-risk AI system. For high-risk AI systems used\
      \ for law enforcement purposes Аrticle-13 of Directive (EU) 2016/680 shall apply;\n-\tcooperate with competent authorities.\
      \    \n\nIf you have selected several sectors within Annex III and one of the use cases is considered high-risk and\
      \ the other(s) not, compliance with the whole Chapter 3 applies only to the high-risk use case you have selected and\
      \ the requirements must be assessed in view of that intended purpose"
  flag_ai_system_no_highrisk_frimpactassessement_provider_output:
    id: flag_ai_system_no_highrisk_frimpactassessement_provider_output
    description: "According to Article 6 (4), as a provider, if you consider that your AI system, as referred to in Annex\
      \ III, is not high-risk, you must document this assessment before the system is placed on the market or put into service.\
      \ Upon request from national competent authorities, you must provide the documentation of the assessment. You are also\
      \ subject to the registration obligation outlined in Article 49(2). \nAttention, an AI system referred to in Annex III\
      \ shall always be considered to be high-risk where the AI system performs profiling of natural persons."
  flag_fr_impact_assessment_deployer:
    id: flag_fr_impact_assessment_deployer
    description: "Prior to deploying a high-risk AI system, deployers that are bodies governed by public law, or are private\
      \ entities providing public services, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of Annex\
      \ III, shall perform an assessment of the impact on fundamental rights that the use of such system may produce. For\
      \ that purpose, deployers shall perform an assessment based on Article 27 requirements.\nThe above obligation applies\
      \ to the first use of the high-risk AI system. The deployer may, in similar cases, rely on previously conducted fundamental\
      \ rights impact assessments or existing impact assessments carried out by provider. \nOnce the assessment has been performed,\
      \ the deployer shall notify the market surveillance authority of its results.\nIf any of the obligations laid down in\
      \ this Article is already met through the data protection impact assessment conducted pursuant to Article-35 of Regulation\
      \ (EU) 2016/679 or Article-27 of Directive (EU) 2016/680, the fundamental rights impact assessment referred to in paragraph\
      \ 1 of this Article shall complement that data protection impact assessment."
  flag_obligation_transparency_provider:
    id: flag_obligation_transparency_provider
    description: "Following Article 50 of the AI Act, you must follow certain transparency requirements as a provider.\n-\
      \ If your AI system interacts directly with natural persons (e.g. a chatbot), you must ensure that AI systems are designed\
      \ and developed in such a way that the natural persons concerned are informed that they are interacting with an AI system.\
      \ \nUnless this is obvious from the point of view of a natural person who is reasonably well-informed, observant and\
      \ circumspect, taking into account the circumstances and the context of use. \n\nThis obligation does not apply to AI\
      \ systems authorised by law to detect, prevent, investigate or prosecute criminal offences, subject to appropriate safeguards\
      \ for the rights and freedoms of third parties, unless those systems are available for the public to report a criminal\
      \ offence.\n\n- If your AI system generates synthetic audio, image, video or text content you must ensure that the outputs\
      \ of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulated.\
      \ You must ensure that your technical solutions are effective, interoperable, robust and reliable as far as this is\
      \ technically feasible, taking into account the specificities and limitations of various types of content, the costs\
      \ of implementation and the generally acknowledged state of the art, as may be reflected in relevant technical standards.\
      \ \n\nTransparency obligations shall not apply to the extent the AI systems perform an assistive function for standard\
      \ editing or do not substantially alter the input data provided by the deployer or the semantics thereof, or where authorised\
      \ by law to detect, prevent, investigate or prosecute criminal offences. \nThe information shall be provided to the\
      \ natural persons concerned in a clear and distinguishable manner at the latest at the time of the first interaction\
      \ or exposure. The information shall conform to the applicable accessibility requirements."
  flag_obligation_transparency_deployer:
    id: flag_obligation_transparency_deployer
    description: "Following Article 50 of the AI Act, you must follow certain transparency requirements as a deployer.\n-\t\
      If you are deploying an emotion recognition system or a biometric categorisation system, you must inform the natural\
      \ persons exposed thereto of the operation of the system, and shall process the personal data in accordance with Regulations\
      \ (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable. This obligation does not apply to AI\
      \ systems used for biometric categorisation and emotion recognition, which are permitted by law to detect, prevent or\
      \ investigate criminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, and\
      \ in accordance with Union law. \n\n-\tIf your AI system generates or manipulates image, audio or video content constituting\
      \ a deep fake, you must disclose that the content has been artificially generated or manipulated. This obligation shall\
      \ not apply where the use is authorised by law to detect, prevent, investigate or prosecute criminal offence. Where\
      \ the content forms part of an evidently artistic, creative, satirical, fictional or analogous work or programme, the\
      \ transparency obligations set out in this paragraph are limited to disclosure of the existence of such generated or\
      \ manipulated content in an appropriate manner that does not hamper the display or enjoyment of the work.\n\n-\tIf your\
      \ AI system generates or manipulates text which is published with the purpose of informing the public on matters of\
      \ public interest shall disclose that the text has been artificially generated or manipulated. This obligation shall\
      \ not apply where the use is authorised by law to detect, prevent, investigate or prosecute criminal offences or where\
      \ the AI-generated content has undergone a process of human review or editorial control and where a natural or legal\
      \ person holds editorial responsibility for the publication of the content. \n\nThe information should be provided to\
      \ the natural persons concerned in a clear and distinguishable manner at the latest at the time of the first interaction\
      \ or exposure. The information shall conform to the applicable accessibility requirements."
  flag_obligations_aisystem_opensource:
    id: flag_obligations_aisystem_opensource
    description: The AI Act does not apply to AI systems released under free and open-source licences, unless they are placed
      on the market or put into service as high-risk AI systems or as an AI system that falls under Article 5 (prohibited
      AI systems) or Article 50 (transparency obligations). See Article 2 (12).
  flag_obligations_aisystem_nohighrisk_output:
    id: flag_obligations_aisystem_nohighrisk_output
    description: Your AI system is likely not considered as high-risk. It is likely that your AI system does not have to comply
      with Chapter 3 of the AI Act, which relates to high-risk AI systems.
  flag_obligations_authorisedrep:
    id: flag_obligations_authorisedrep
    description: 'Following Article 54 of the AI Act, the authorised representative shall perform the tasks specified in the
      mandate received from the provider. It shall provide a copy of the mandate to the AI Office upon request. For the purposes
      of the AI Act, the mandate shall empower the authorised representative to carry out the following tasks:


      - verify that the technical documentation specified in Annex XI has been drawn up and all obligations referred to in
      Article 53 and, where applicable, Article 55 have been fulfilled by the provider;


      - keep a copy of the technical documentation specified in Annex XI at the disposal of the AI Office and national competent
      authorities, for a period of 10 years after the general-purpose AI model has been placed on the market, and the contact
      details of the provider that appointed the authorised representative;


      - provide the AI Office, upon a reasoned request, with all the information and documentation, including that referred
      to in the previous point, necessary to demonstrate compliance with the obligations in the AI Act;


      - cooperate with the AI Office and competent authorities.


      The authorised representative shall terminate the mandate if it considers or has reason to consider the provider to
      be acting contrary to its obligations pursuant to the AI Act. In such a case, it shall also immediately inform the AI
      Office about the termination of the mandate and the reasons therefor.'
  flag_ai_system_obligations_productmanufacturer:
    id: flag_ai_system_obligations_productmanufacturer
    description: If you are a product manufacturer and do not fall under the conditions of Article 25(3) of the AI Act, you
      likely do not have obligations under the AI Act.
  flag_outofscope_gpai:
    id: flag_outofscope_gpai
    description: You are likely not a provider or an authorised representative of a provider under the AI Act, so you are
      not subject to the obligations for these roles.
  flag_obligations_provider_section_b_results_high_risk:
    id: flag_obligations_provider_section_b_results_high_risk
    description: Your AI system is likely classified as a high-risk system under the AI Act, following Article 2 (2) and Article
      6 (1). You are required to comply with the requirements outlined in Article 8 to Article 15 of the AI Act; however,
      these articles need to be transposed into the relevant sectoral legislation, in accordance with Article 102 to Article
      109, depending on the sectoral legislation applicable to the case at hand.
  flag_obligations_authorisedrep_nosystemicrisk_opensource:
    id: flag_obligations_authorisedrep_nosystemicrisk_opensource
    description: Your general-purpose AI model likely qualifies for the open-source exception. Pursuant to Article 54 (6)
      AI Act there are no obligations for authorised representatives of an open-source, general-purpose AI model.
  flag_downstream_modifier:
    id: flag_downstream_modifier
    description: The obligations under Article 53(1) AI Act are limited to the modifications, see Section 3.2.1 of the European
      Commission's [guidelines on the scope of the obligations for general-purpose AI models](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-07/guidelines_on_the_scope_of_the_obligations_for_generalpurpose_ai_models_established_by_regulation_1cx2atxgq79us4n3x8jfgyy1qlm_118340-3.pdf#page=21).
obligations: {}
